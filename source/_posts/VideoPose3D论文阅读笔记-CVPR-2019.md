---
title: VideoPose3D论文阅读笔记-CVPR 2019
toc: true
mathjax: true
tags:
  - 论文
  - CVPR
  - 3D人体姿态估计
categories: 文献阅读笔记
abbrlink: 40498
date: 2023-02-16 20:48:35
---

论文题目为3D human pose estimation in video with temporal convolutions and semi-supervised training，发表于CVPR 2019。本文提出了时间扩张卷积模型，并使用了半监督的方法。

<!-- more -->

# 大纲

<img src="https://user-images.githubusercontent.com/56388518/219369406-07741ca7-0ac3-481a-b584-57f971798128.png" alt="" style="zoom: 50%;" />

# 引言

先前的工作依赖RNN建模时间信息来解决深度歧义。另一方面，卷积网络在建模时间信息方面非常成功，且卷积模型能够并行处理多个帧，而循环网络不可能实现。

本文提出了一种全卷积架构，该架构在2D关键点上执行时间卷积，以实现视频中的准确3D姿态预测。该方法与任何2D关键点检测器兼容，并可以通过扩展卷积有效地处理大量的上下文信息。与依赖RNN的方法相比，它在计算复杂度和参数数量方面有着更高的准确性、简单性和高效性。

同时本文利用未标记的视频数据进行半监督训练，该方法使用现成的2D关键点检测器预测未标记视频的2D关键点，接着预测3D姿态，然后将这些姿态映射回2D空间。

总结本文的两个贡献：

- 提出了一种基于2D关键点轨迹的扩展时间卷积的视频3D人体姿态估计方法。在相同精度水平下，无论是在计算复杂性还是模型参数数量方面，均比基于RNN的模型更有效。
- 引入了一种半监督方法，该方法利用未标记的视频。与以前的半监督方法相比，该方法只需要相机的固有参数，而不需要真实2D注释或具有外部相机参数的多视图图像。

<img src="https://user-images.githubusercontent.com/56388518/219369529-71a905e1-f1b1-49c6-937b-34a4659817af.png" alt="" style="zoom: 50%;" />

<center>图1. 时间卷积模型</center>

# 相关工作

第一个使用CNN的方法专注于端到端的重建，通过从RGB图像直接估计3D姿态，无需中间监督。

## 两阶段姿态估计

新一批的3D姿态估计器建立在2D姿态估计的基础上，然后将其提升到3D。受益于中间监督，这些方法优于端到端的方法。本文遵循这种方法。

早期方法在3D姿态可用的一大组2D关键点上执行对2D关键点预测集的k近邻搜索，然后输出相应的3D姿态。一些方法同时利用图像特征和2D真实姿态。或者通过简单预测2D关键点的深度，从给定的一组2D关键点预测3D姿态。还有一些工作利用了关于骨骼长度和投影与2D真实姿态一致性的先验知识。

## 视频姿态估计

以前的大多数工作都是基于单帧图像的，但最近有些工作利用视频中的时间信息来产生更具鲁棒性的预测，并降低对噪声的敏感度。

如利用时空体积的HoG（histograms of oriented gradients，定向梯度直方图）推断3D姿态。LSTM已用于细化从单帧图像预测的3D姿态。然而最成功的方法是从2D关键点轨迹中学习，本文的工作属于这一类。

最近，已经提出了LSTM Seq2Seq学习模型，该模型将视频中的2D姿势序列编码为固定大小的向量，然后将其解码为3D姿势序列。然而，输入和输出序列都具有相同的长度，2D姿态的确定性变换是更自然的选择。本文用seq2seq模型进行的实验表明，输出姿势往往会在较长的序列上漂移。有些工作通过以时间一致性为代价每5帧重新初始化编码器来解决这个问题。也有关于RNN方法的工作，该方法考虑了身体部位连接的先验。

## 半监督训练

有人提出多任务网络的工作，用于联合2D和3D姿态估计以及动作识别。一些工作将在2D姿势估计学习到的特征转移到3D任务中。未标记的多视图记录已用于3D姿态估计的预训练表示，但这些记录在无监督设置中不易获得。生成对抗网络（GAN）可以在只有2D注释可用的第二数据集中区分现实姿态和不现实姿态，从而提供了一种有用的正则化形式。还有的工作使用GAN从未配对的2D/3D数据集学习，并包括2D投影一致性项。类似地，另一个工作在将生成的3D姿势随机投影到2D之后，对其进行区分。还有人提出了一种基于有序深度注释的弱监督方法，该方法利用了2D姿势数据集，并通过深度比较进行了增强，例如“左腿在右腿后面”。

## 3D形状恢复

虽然本文和所讨论的相关工作侧重于重建准确的3D姿态，但并行的研究目标是从图像中恢复人的完整3D形状。这些方法通常基于参数化的3D网格，对姿态精度的要求较小。

## 本文工作

与Pavlakos等人的方法相比，本文不使用热图，而是使用检测到的关键点坐标来描述姿势。这允许在坐标时间序列上使用有效的1D卷积，而不是在单个热图上使用2D卷积，或在热图序列上使用3D卷积。本文方法还使计算复杂性与关键点空间分辨率无关。

本文提出的模型可以用更少的参数达到高精度，并允许更快的训练和推断。与Martinez等人提出的单帧基线和Hossain等人提出的LSTM模型相比，我们通过在时间维度上执行1D卷积来利用时间信息，并提出了拥有较低重建误差的若干优化。与Hossain等人不同，我们学习的是确定性映射，而不是Seq2Seq模型。

与本节中提到的大多数两步模型相反，我们表明掩码R-CNN和级联金字塔网络（Cascaded Pyramid Network, CPN）检测对于3D人体姿态估计更为鲁棒。

# 时间扩张卷积模型

本文的模型是一个具有残差连接的完全卷积架构，它将一系列2D姿态作为输入，并通过时间卷积对其进行变换。

卷积模型可以在批处理和时间维度上实现并行化，而RNN不能随时间并行化。在卷积模型中，无论序列长度如何，输出和输入之间的梯度路径都具有固定长度，这减轻了RNN中梯度消失和爆炸的情况。卷积结构还提供了对时间感受野的精确控制，作者发现这有益于3D姿态估计任务中的时间依赖性建模。此外，作者使用扩张卷积来建模长期相关性，同时保持效率。

输入层获取每帧的$J$个关节的级联坐标$(x,y)$，并用具有卷积核大小为$W$和$C$的输出通道的时间卷积。接着是$B$个由skip-connection包围的ResNet-style块。每个块首先执行卷积核大小为$W$和膨胀因子为$D=W^B$的1D卷积，接着执行卷积核大小为1的卷积。除了最后一层的卷积，其余卷积层后面都跟着batch normalization、ReLU和Dropout。

每个块都会使感受野以指数方式增长$W$倍，而参数数量只会线性增加。设置滤波器超参数$W$和$D$使得任何输出帧的感受野形成包含所有输入帧的树，如图1所示。最后一层同时利用过去和未来的数据来利用时间信息为所有输入序列中的帧输出一个3D姿态预测。为了评估实时场景，本文还实验了因果卷积，即只能访问过去帧的卷积。

图2为该架构的一个实例，该架构的感受野为243帧，$B=4$个块。卷积层设置$W=3,C=1024$个输出通道，dropout概率为$p=0.25$。关键点$J=17$个关节。其中绿色为卷积层，$2J,3d1,1024$表示$2J$个输入通道，卷积核大小为3，扩张率为1，1024个输出通道。括号中的数字表示样本1帧预测的张量大小，$(243,34)$表示243帧和34个通道。为了保证卷积运行的有效性，对残差（左右对称）进行切片以匹配后续张量的形状。

![](https://user-images.githubusercontent.com/56388518/219369641-328b98d3-3fbd-4194-a237-5791d69f46a6.png)

<center>图2. 全卷积3D姿态估计架构实例</center>

# 半监督方法

本文将未标记的视频与现成的2D关键点检测器相结合，以扩展具有反向投影损失项的监督损失函数。并解决了未标记数据的自编码问题。编码器（姿态估计器）利用2D关节坐标进行3D姿态估计，解码器（投影层）将3D姿态投影回2D关节坐标。当解码器投影的2D关节坐标远离原始输入坐标时，训练将受到惩罚。

图3为本文的方法，该方法将监督组件和非监督组件相结合，非监督组件充当正则化器。这两个目标是联合优化的，一个batch的前半部分是标记的数据，后半部分是未标记的数据。对于标记的数据，使用真实3D姿态作为目标，并训练监督损失。未标记的数据用于实现自编码器的损失，将预测的3D姿态投影回2D后检查与输入的一致性。

<img src="https://user-images.githubusercontent.com/56388518/219369740-79bac450-4215-43c1-9dda-a2930aa3786c.png" alt="" style="zoom:50%;" />

<center>图3. 使用以预测的2D姿态序列作为输入的3D姿态模型进行半监督训练</center>

## 轨迹模型

屏幕上的2D姿态取决于轨迹和3D姿态，即人类根关节的全局位置和其他关节相对于根关节的位置。如果没有全局位置，对象将始终以固定的比例重投影在屏幕中心。故本文还回归了人的3D轨迹，以便可以正确的执行到2D空间的反向投影。为此，本文优化了第二个网络，该网络回归了相机空间中的全局轨迹，并在将姿态投影回2D之前，将全局轨迹添加到姿态中。

轨迹模型和姿态估计模型具有相同的架构，但不共享任何权重，因为以多任务方式进行训练时，它们会对彼此产生负面影响。由于受试者距离相机越远，回归精确轨迹就越困难，所以本文为轨迹优化了一个加权平均每关节位置误差（WMPJPE）损失函数：
$$
E=\frac{1}{ {\bf y}_z}||f({\bf x})-{\bf y}||
$$
其中${\bf y}_z$为相机空间中的真实深度，则上式即为使用该真实深度的倒数对每个样本进行加权。对本文来说，回归与相机距离较远对象的精确轨迹不是必要的，因为相应的2D关键点往往集中在一个小区域周围。

## 骨长度L2损失

本文希望鼓励模型对可能的3D姿态进行预测，而不是单纯的复制输入。为此，本文发现添加软约束是一个有效的方法，该约束使得未标记数据中受试者的平均骨长度与标记数据中受试者的大致匹配。该约束项在自监督中发挥着重要作用。

## 讨论

本文的方法只需要摄像机的固有参数，这些参数在通常的商业摄像机上均可获得。该方法不受任何特定网络架构的限制，可应用于任何以2D关键点为输入的3D姿态检测器。在上述提到的反向投影的过程中，使用了一个简单的投影层，该层考虑了线性系数（焦距、主点）和非线性透镜畸变系数（切向的和径向的）。虽然发现Human3.6M中使用的相机镜头失真对姿态估计的影响可以忽略不计，但本文的方法中仍包含了这些项，因为它们可以提供更加精确的真实相机投影建模。

# 实验设置

## 数据集和评价指标

本文使用了Human3.6M和HumanEva-I两个数据集。

本文实验考虑了三种评估协议：

- MPJPE，即平均每关节位置误差，这是预测关节位置和真实关节位置之间的平均欧几里德距离
- P-MPJPE，即对数据进行平移、旋转、缩放后与真实位置对齐后的MPJPE
- N-MPJPE，即按比例将预测关节位置与真实关节位置对齐后的MPJPE

## 2D姿态估计的实现细节

大多数先前的工作从真实边界框中提取对象，然后应用基于堆叠沙漏网络的检测器来预测真实边界框内的2D关键点位置。而本文的方法不依赖于任何特定的2D关键点检测器。

作者研究了几种不依赖真实边界框的2D检测器，可以在户外场景中使用。作者研究了以ResNet-101-FPN作为backbone的Mask R-CNN，使用其在Detectron中的实现。以及FPN扩展的代表——级联金字塔网络（Cascaded Pyramid Network, CPN）。CPN实现需要外部提供边界框，在此情况下，本文使用了Mask R-CNN边界框

对于Mask R-CNN和CPN，从COCO上的预训练模型开始，并在Human3.6M的2D投影上微调检测器。

对于Mask R-CNN，作者采用ResNet-101作为backbone，并采用拉伸1倍（stretched 1x）的策略进行训练。在Human3.6M上微调时，作者重新初始化了关键点网络的最后一层，以及回归热图的去卷积层（deconvolution layer），以此来学习一组新的关键点。在4个GPU上以逐步衰减的学习率进行训练：先以1e-3的学习率迭代6万次，再以1e-4的学习率迭代1万次，最后以1e-5的学习率迭代1万次。推断阶段，在热图上使用softmax，并提取所得2D分布的预期值（soft argmax）。这将带来比hard-argmax更平滑、更精确的预测。

对于CPN，作者采用分辨率为$384\times288$的ResNet-50作为backbone。为了进行微调，作者重新初始化了GlobalNet和RefineNet的最终层（卷积权重和批量规范化统计）。在单个GPU上以32的批量大小和逐步衰减的学习率进行训练：先以5e-5的学习率迭代6千次，再以5e-6迭代4千次，最后以5e-7的学习率迭代2千次。作者在微调时启用了批量规范化（batch normalization）。使用真实边界框进行训练，并使用微调后的Mask R-CNN模型预测的边界框进行测试。

## 3D姿态估计的实现细节

为了和其他工作保持一致，作者在相机空间中进行3D姿态的训练和评估时仅根据相机变换旋转、平移真实姿态，而不使用全局轨迹（涉及半监督时除外）。

作者使用Amsgrad作为优化器，训练80轮。对于Human3.6M，使用指数衰减学习率策略，从$\eta=0.001$开始，每个epoch的收缩因子均为$\alpha=0.95$

> 指数衰减学习率：
> $$
> \eta=\eta_0*\alpha^{epoch/b}
> $$
> 其中，$\eta_0$为初始学习率，$\alpha$为收缩因子，$epoch$为当前训练轮次，$b$表示每$b$轮衰减一次

所有时间模型，即感受野大于1的模型，对姿态序列中样本的相关性是敏感的。这将导致批量规范化中存在统计偏差，因为BN默认假设样本是独立的。作者在初步实验中发现，在训练期间预测大量相邻帧产生的结果比不利用时间信息的模型（在批处理中具有良好随机化的样本）更差。作者通过从不同的视频片段中选取训练clip来减少样本之间的相关性。clip集大小设置为本文架构的感受野宽度，以便模型预测每个训练clip的单个3D姿态。

可以通过用跨步卷积代替扩张卷积来极大的优化上述单帧场景，其中步幅设置为扩展因子（附录A.6）。这样可以避免未使用过的计算状态，并且仅在训练期间使用此优化。在推理时，可以处理整个序列，并重用其他3D帧的中间状态，以便更快地进行推理。这是可行的，因为本文的模型不在时间维度上使用任何的池化。为了避免帧丢失以及保证卷积的合法性，本文通过复制来进行填充，但只在序列的边界处进行这种填充。

作者观察到，批量规范化的默认超参数会导致测试误差的大幅波动，同时也会导致用于推断的运行时估计值的大幅波动。为了获得更稳定的运行时统计数据，作者为批量规范化的momentum参数$\beta$制定了一个策略：从$\beta=0.1$开始，并按指数衰减，使其在最后一个epoch达到$\beta=0.001$。

最后，在训练和测试时执行水平翻转增强。

# 结果

## 时间扩张卷积模型

![](https://user-images.githubusercontent.com/56388518/219369826-740faa77-914d-4ad1-a1c8-4363a05370e4.png)

![](https://user-images.githubusercontent.com/56388518/219369833-16261f2a-0207-4435-ad85-4af7259a7394.png)

<center>表1. Human3.6M数据集上的重建误差</center>

$(\textdagger)$表示该方法利用了时间信息；$(*)$使用了真实边界框；$(+)$使用了额外数据。粗体是最好的，下划线是第二好的。

有趣的是，真实边界框的性能与Mask R-CNN的预测边界框相似，这表明在本文的单帧场景中，预测几乎是完美的。

![](https://user-images.githubusercontent.com/56388518/219369932-1028ad97-ecde-4dd8-b8b0-511118188e79.png)

<center>图4. 包括预测的2D关键点在内的预测姿势的示例</center>

<img src="https://user-images.githubusercontent.com/56388518/219369986-2c612bbe-c8ad-4af1-b870-f70c053362d1.png" alt="" style="zoom:50%;" />

<center>表2. 2D关键点检测器对最终结果的影响</center>

(GT)表示ground-true；(SH)表示stacked hourglass，即堆叠沙漏网络；(D)表示Detectron；(CPN)表示cascaded pyramid network，即级联金字塔网络；(PT)表示pre-trained；(FT)表示fine-tuned，即微调过的

绝对位置误差不能测量预测随时间的平滑度，这对视频很重要。为了评估这一点，作者测量了与3D姿态序列的一阶导数的MPJPE相对应的关节速度误差（MPJVE）。

![](https://user-images.githubusercontent.com/56388518/219370091-bd972475-272c-4add-93b3-43f652619b38.png)

<center>表3. 时间扩展卷积与单帧baseline的MPJVE对比</center>

表3展示了时间扩展卷积与单帧baseline的MPJVE对比，相比提升了76%，从而使姿态更加平滑。

<img src="https://user-images.githubusercontent.com/56388518/219370194-19fc4ca8-f1ac-4012-85fa-7c734d1a212a.png" alt="" style="zoom:50%;" />

<center>表4. HumanEva-I的结果</center>

表4展示了HumanEva-I的结果，并且该模型推广到较小的数据集。结果基于预训练的Mask R-CNN 2D检测。本文的模型超越了当时的SOTA。

<img src="https://user-images.githubusercontent.com/56388518/219370398-564bf2ac-0595-408a-b992-b35e36947120.png" alt="" style="zoom:50%;" />

<center>表5. 卷积模型和LSTM模型的复杂性对比</center>

本文的最大模型具有243帧的感受野，其复杂度与该LSTM模型大致相同，但误差较低。该表还强调了扩张卷积的有效性，扩张卷积仅以对数方式增加了接受野的复杂性，详见表5。

由于本文的模型是卷积的，因此它可以在序列数量和时间维度上并行化，对于小批量来说效率要比RNN高得多。

## 半监督方法

本文将Human3.6M训练集的各种子集视为标记数据，剩余样本用作未标记数据。并将所有数据从50FPS降采样到10FPS。

由于数据集是下采样的，本文使用9帧的感受野，相当于上采样的45帧。对于非常小的子集使用3个帧。并仅在标记数据上微调CPN，通过在标记的数据上迭代几个epoch来预热训练。

<img src="https://user-images.githubusercontent.com/56388518/219370522-280aa9f1-9062-4850-8490-1340e79f369d.png" alt="" style="zoom: 50%;" />

<center>图5a. 标记数据数量与半监督方法效果的关系（协议3下降采样到10FPS）</center>

图5a显示，随着标记数据量的减少，本文的半监督方法变得更加有效

<img src="https://user-images.githubusercontent.com/56388518/219370569-52ca05b3-381e-4488-9ebe-b5e86270b870.png" alt="" style="zoom: 50%;" />

<center>图5b. 协议1下对于数据集的非下采样版本（50 FPS）的结果</center>

图5b中描述的设置更适合本文的方法，因为它允许模型利用视频中的全部时间信息，这里使用27帧的感受野。本文的半监督方法比监督基线提高了14.7mm MPJPE。

<img src="https://user-images.githubusercontent.com/56388518/219370606-769545e6-70ce-4da5-a9b2-5395ecae7382.png" alt="" style="zoom:50%;" />

<center>图5c. 将CPN 2D关键点切换为真实2D姿态</center>

图5c证实了更好的2D检测可以提高性能。并且图中展示了移除了骨长度约束的半监督方法的效果（Ours semi-supervised GT abl.），结果显示删除该项会大大降低半监督训练的有效性。

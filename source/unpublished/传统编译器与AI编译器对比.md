---
title: 传统编译器与AI编译器对比
toc: true
mathjax: true
date: 2024-03-26 11:30:55
tags:
categories:
---

摘要

<!-- more -->

## 基本概念

### 编译器与解析器

|          | Interpreter                                                  | Compiler                                                     |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 程序步骤 | 1. 创建代码<br />2. 没有文件链接或机器代码生成<br />3. 源语句在执行过程中逐步执行 | 1. 创建代码<br />2. 解析或分析所有语言语句的正确性<br />3. 将源码转换为机器码<br />4. 链接到可执行程序<br />5. 运行程序 |
| Input    | 每次读取一行                                                 | 整个程序                                                     |
| Output   | 不产生任何中间代码                                           | 生成中间目标代码                                             |
| 工作机制 | 编译和执行同时进行                                           | 编译在执行之前完成                                           |
| 存储     | 不保存任何机器代码                                           | 在机器上存储编译后的机器代码                                 |
| 执行     | 程序执行是解释过程的一部分，逐行执行                         | 程序执行与编译是分开的，在整个输出程序编译后执行             |
| 生成程序 | 不生成输出程序，每次执行过程都要评估源程序                   | 生成可以独立于源程序运行的输出程序                           |
| 修改     | 可以直接修改                                                 | 修改后需要重新编译                                           |
| 运行速度 | 较慢                                                         | 较快                                                         |
| 内存     | 所需内存较少，因为不创建中间代码                             | 由于要创建目标代码，所需内存更多                             |
| 错误     | 解释器读取一条语句并显示错误，纠正后才能解释下一行           | 编译器在编译时显示所有错误和警告，不修正就不能运行程序       |
| 错误监测 | 较为容易                                                     | 较难                                                         |
| 编程语言 | PHP、Perl、Python、Ruby                                      | C、C++、C#、Scala、Java                                      |

### JIT和AOT编译方式

程序主要有两种运行方式：

- 静态编译：程序在执行前全部被编译为机器码，称为AOT（Ahead of time），也就是提前编译；
- 动态解释：程序一边编译一边运行，称为JIT（Just in time），也就是即时编译；

|      | JIT                                                          | AOT                                                          |
| ---- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 优点 | 1. 可以根据当前硬件情况实时编译生成最优机器指令<br />2. 可以根据当前程序的运行情况生成最优的机器指令序列<br />3. 当程序需要支持动态链接时，只能使用JIT<br />4. 可以根据进程中内存的实际情况调整代码，能够更充分的利用内存 | 1. 程序运行前编译，可以避免运行时的编译性能消耗和内存消耗<br />2. 可以在程序运行初期就达到最高性能<br />3. 可以显著加快程序的启动 |
| 缺点 | 1. 编译需要占用运行时资源<br />2. 编译占用运行时间，对某些代码编译优化不能完全支持，需要在流畅和时间之间权衡<br />3. 在编译准备和识别频繁使用的方法需要占用时间，初始编译不能达到最高性能 | 1. 程序运行前编译会使程序安装时间增加<br />2. 牺牲高级语言的一致性问题<br />3. 将提前编译的内容保存会占用更多的外存 |

一般在训练过程中使用JIT模式，离线推理过程中使用AOT模式。

### Pass和中间表达IR

Pass是对源程序的一次完整扫描或处理，一次编译过程可能有多个Pass。

IR（Intermediate representation）是编译器或虚拟机内部用来表示源代码的数据结构或代码。

## 传统编译器

### 编译器基本构成

- 前端（Front-End）：负责词法和语法分析，将源代码转化为抽象语法树；
- 优化器（Optimizer）：对前端得到的中间代码进行优化，使代码更加高效；
- 后端（Back-End）：将优化过的中间代码转化为针对各自平台的机器代码。

### GCC编译过程和原理

GCC是一个可移植的编译器，支持多种硬件平台。它不仅仅是本地编译器，还能跨平台交叉编译。有多种语言前端，用于解析不同的语言。采用模块化设计，可以加入新的语言和新CPU架构支持。

#### 编译过程

![](https://github.com/Deleter-D/Images/assets/56388518/928e4caf-3a39-47ed-bd48-ae350f04fdf2)

- 预处理

包括宏定义、文件包含、条件编译三个部分。预处理过程读入源代码，检查包含预处理指令的语句和宏定义，并对其进行响应和替换。预处理过程会删除进程中的注释和多余空白字符。最后生成`.i`文件。

```shell
gcc -E hello.c -o hello.i
```

- 编译

编译器对`.i`文件进行语法分析并优化，然后生成对应的汇编代码`.s`文件。 

```shell
gcc -S hello.i -o hello.s
```

- 汇编

汇编器将编译器生成的`.s`汇编程序汇编为机器语言或指令，也就是机器可以执行的二进制程序`.o`文件。

```shell
gcc -c hello.s -o hello.o
```

- 链接

连接器会链接程序运行所需要的目标文件，以及依赖的库文件，最后身成可执行文件，以二进制形式存在磁盘中。

```shell
gcc hello.o -o hello
```

#### 优缺点

优点：

- 支持Java、ADA、Fortran等多种语言；
- 支持更多平台；
- 更流行，使用范围更广，支持较为完备；
- GCC基于C，不需要C++编译器即可编译。

缺点：

- GCC代码耦合度高，很难独立，若集成到IDE中，难以用模块化的方式调用GCC；
- GCC被构建成单一静态编译器，难以作为API集成到其他工具中；
- 越后期的版本，代码质量越差，代码量约1500万行，过于庞大。

### LLVM架构和原理

LLVM的一大贡献就是提出了中间表达，将所有语言前端都对接到IR，以IR这种统一的方式对接到各种硬件。

LLVM的一大理念就是Lib base，即基于库的实现。LLVM主要有以下库：

- LLVM Core：LLVM的核心库，主要是围绕LLVM中间代码的一些工具，提供了一个“源”和“目标”无关的优化器，以及几乎所有主流CPU类型的机器码生成器；
- Clang：LLVM的一个子项目，基于LLVM架构的轻量级编译器，负责编译C、C++、Objective-C语言，属于LLVM架构中的编译器前端；
- Compiler-RT：为硬件不支持的低级功能提供特定于目标的支持；
- LLDB：LLVM的原生调试器项目，提供丰富的流程控制和数据检测的调试功能；
- LLD：Clang/LLVM内置的连接器；
- Dragonegg：GCC插件，可以将GCC的优化和代码生成器替换为LLVM的相应工具；
- libc：C标准库实现；
- libcxx/libcxxabi：C++标准库实现；
- libclc：OpenCL标准库实现；
- OpenMP：提供OpenMP运行时，用于Clang中的OpenMP实现；
- polly：支持高级别的循环和数据本地化优化支持的LLVM框架，使用多面体模型实现一组缓存局部优化以及自动并行和矢量化；
- vmkit：基于LLVM的Java和.Net虚拟机实现；
- klee：基于LLVM编译基础设施的符号化虚拟机，使用一个定理证明器来尝试评估程序中的所有动态路径，以发现错误并证明函数的属性。klee的一个主要特征是它可以在检测到错误时生成测试用例；
- SAFECode：用于C/C++程序的内存安全编译器，通过运行时来检查代码，以便检测内存安全错误，可以用于保护软件免受安全攻击，也可用作Valgrind等内存安全错误调试工具。

![](https://github.com/Deleter-D/Images/assets/56388518/22b71010-d4f1-481b-b56e-405055c9fb39)

#### 编译过程

![](https://github.com/Deleter-D/Images/assets/56388518/cd27efa0-7470-4204-88ed-8759c0c3b04d)

- 预处理

```shell
clang -E -c hello.c -o hello.i
```

- 编译

可以导出字节码文件`.bc`，即IR中间表示。

```shell
clang -emit-llvm hello.i -c -o hello.bc
```

也可以导出可以识别的`.ll`文件。

```shell
clang -emit-llvm hello.i -S -o hello.ll
```

- 优化Pass并生成汇编代码

```shell
llc hello.ll -o hello.s
```

- 汇编并生成目标文件

```shell
clang -c hello.s -o hello.o
```

- 链接

```shell	
clang hello.o -o hello
```

#### IR详解

IR并不指一种固定的形态，LLVM架构下，编译器前端会将AST形式的IR传给优化器，优化器的每一个Pass之间都会有相同或者不同形式的IR，最后以DAG形式的IR传递给后端，后端可能再转变IR的形式进行优化。

LLVM IR作为一种编译器IR，有两个基本原则指导核心库的开发：

- SSA表示，代码组织为三地址指令序列和无限寄存器，使优化能够快速进行；
- 整个程序的IR存储到磁盘中，让链接时优化易于实现。

> SSA（Static single assignment）即静态单赋值形式，有两个重要特征：
>
> - 代码组织为三地址指令序列；
> - 寄存器数量无限制，是为了和硬件解耦。

SSA程序的每个变量有且只有一个赋值语句，在LLVM IR中，每个变量都必须在使用前定义，且每个变量只能被赋值一次。使用SSA的好处是，每次使用一个值，可以立即向后追溯到给出其定义的唯一指令。还可以极大的简化优化，因为SSA形式建立了平凡的use-define链，也就是一个值到达使用处的定义的列表。

##### LLVM IR表示形式

LLVM IR有三种具体表示形式，三种中间格式是完全等价的：

- 在内存中的编译中间语言，例如无法通过文件的形式得到的指令类等；
- 在硬盘上存储的二进制中间语言，格式为`.bc`；
- 人类可读的代码语言，格式为`.ll`。

##### LLVM IR内存模型

- LLVM IR文件的基本单位称为module；
- 一个module可以拥有多个顶层实体，如function和global variable；
- 一个function define中至少有一个basicblock；
- 每个basicblock中有若干instruction，并都以terminator instruction结尾。

#### LLVM前端

编译器前端将源代码转化为编译器的中间表示LLVM IR。

- 词法分析：处理源代码的文本输入，将语言结构分解为一组单词和标记，去除注释、空白、制表符等。每个单词或标记必须属于语言子集，语言的保留字被变换为编译器内部表示；
- 语法分析：分组标记以形成表达式、语句、函数体等。检查一组标记是否有意义，考虑代码物理布局，但并不分析代码含义，只保证语法正确性，并输出抽象语法树AST；
- 语义分析：借助符号表检验代码是否违背了语言类型系统。符号表存储标识符和其各自的类型之间的映射，以及其它内容。类型检查的一种直觉的方法是，在解析之后遍历AST的同时从符号表中收集关于类型的信息；

经过上述步骤后，生成LLVM IR，交给优化层进行优化。

#### LLVM优化层

LLVM优化层的输入是LLVM前端生成的IR，具体表现为AST，输出也是IR，具体表现为DAG。

优化层通过一个个的Pass对IR进行优化，Pass分为两类：

- 分析Pass：负责发掘性能和优化的机会；
- 转换Pass：生成必需的数据结构，为后续的Pass所使用。

在转换Pass和分析Pass之间有两种主要依赖类型：

- 显式依赖：转换Pass需要一种分析，则Pass管理器自动地安排它所依赖的分析Pass在它之前运行；
- 隐式依赖：转换或分析Pass要求IR代码运用特定的表达式，需要手动地以正确的顺序将这个Pass加入到Pass队列中，可以通过命令行工具（`clang`或`opt`）或Pass管理器实现。

Pass类是实现优化的主要资源，但并不会直接使用Pass，而是通过清楚的子类来使用它。当实现一个Pass时应该选择合适的Pass粒度，并使用该粒度下的子类。常见的子类有`ModulePass`、`FunctionPass`、`BasicblockPass`等。

#### LLVM后端

整个后端流水线用到了四种不同层次的指令表示：

- 内存中的LLVM IR；
- SelectionDAG结点；
- MachineInstr；
- MCInst。

![](https://github.com/Deleter-D/Images/assets/56388518/04481211-3737-4ecc-b45c-a53e9628278a)

上图中白色的Pass是非必要Pass，灰色的Pass是必须的Pass，也叫Super Pass。

- Instruction selection 指令选择

该Pass将内存中LLVM IR变换为目标特定的Selection DAG结点，每个DAG能够表示单一基本块的计算，结点表示指令，而边编码了指令间的数据流依赖。该Pass使得LLVM代码生成程序库能够运用基于树的模式匹配指令选择算法。

- Instruction Scheduling (I) 指令调度

这是第一次指令调度，也称为前寄存器分配（RA）调度。该Pass对指令进行排序，同时尝试发现尽可能多的指令层次的并行。然后指令被变换为MachineInstr三地址表示。

- Register Allocation 寄存器分配

LLVM IR两个重要特性之一是寄存器集无限，这个性质会一直保持，直到寄存器分配Pass。该Pass将无限的虚拟寄存器引用转换为有限的目标特定的寄存器集合。当寄存器不够时会挤出到内存。

- Instruction Scheduling (II) 指令调度

这是第二次指令调度，也称为后寄存器分配（RA）调度。此时可以获得真实的寄存器信息，某些类型的寄存器存在延迟，可以用来改进指令顺序。

- Code Emission 代码输出

该Pass将指令从MachineInstr表示变换为MCInst实例。这种新的表示更适合汇编器和链接器，可以输出汇编代码或者输出二进制块特定目标代码格式。

## AI编译器

AI编译器与传统编译器的关系如下：

- 目标相同：通过自动化方式进行程序优化和代码生成，从而减少对不同硬件手工优化的工作量；
- 优化方式类似：在编译优化层通过统一IR执行不同的Pass进行优化，从而提高执行性能；
- 软件结构栈类似：分为前端、优化、后端三段式，IR解耦前端和后端使其能够模块化表示；
- AI编译器依赖于传统编译器：AI编译器对Graph IR优化之后，将优化后的IR转换成传统编译器IR，最后依赖传统编译器进行机器码生成。

主要的区别如下：

- IR差异：AI编译器的IR与传统编译器IR所抽象出来的概念和意义不同；
  - AI编译器一般会有High-level IR，用来抽象描述深度学习模型中的运算，如卷积、Matmul等；
  - 传统编译器相对而言是Low-level IR，用于描述基本指令运算，如load、store等。
- 优化策略：AI编译器面向AI领域，优化时会引入更多领域特定知识，从而进行更高级别、更激进的优化手段；
  - AI编译器在High-level执行算子融合，传统编译器执行类似操作时会更保守；
  - AI编译器可以降低计算精度，因为深度学习对精度不那么敏感，但传统编译器一般不执行改变类型和精度等优化。

AI编译器主要分为两个场景：

- 推理场景：输入AI框架训练出来的模型文件，输出能够在不同硬件高效执行的程序；
- 训练场景：输入高级语言表示的神经网络代码，输出能够在不同硬件高效执行的程序。

### AI编译器架构发展

#### 朴素的AI编译器

TensorFlow早期版本，基于神经网络的编程模型，主要进行了graph图和ops算子两层抽象。

- 图层：通过声明式编程方式，以静态图方式执行，执行前进行硬件无关和硬件相关的编译优化；
  - 硬件无关的优化：表达式简化、常量折叠、自动微分等；
  - 硬件相关的优化：算子融合、内存分配等。
- 算子层：采用手写kernel的方式，如在NVIDIA GPU上基于CUDA kernel实现大量算子或依赖cuDNN算子优化库。

这一阶段的编译器在表达上采用静态图方式，静态图的表达式非Python原生，开发者需要使用框架的API进行构图，易用性不好。

在性能上，DSA专用加速芯片的出现加剧了性能挑战；算子层提供的算子粒度和边界提前确定后，无法充分发挥硬件的性能；硬件厂商提供的算子优化库也未必最优。

#### 专用的AI编译器

这个阶段的编译器，在表达上以PyTorch为代表的框架灵活表达API方式称为AI框架的参考标杆，图层的神经网络编译器主要考虑如何将类PyTorch的表达转换到图层的IR进行优化，并试图打开图算边界进行融合优化。

在性能上打开计算图和算子的边界，进行重新组合优化，发挥芯片算力。计算图层下发子图中的算子并打开成小算子，基于小算子组成的子图进行编译优化。

#### 通用的AI编译器

是一个还未到来的阶段，在这一阶段希望解决的问题：

- 图算统一表达，实现融合优化；
- 算子实现自动Schedule、Tiling、Codegen，降低开发门槛；
- 更泛化优化能力，实现动静统一、动态Shape、稀疏性、高阶微分、自动并行等；
- 包括编译器、运行时、异构计算、边缘到数据中心都模块化表示和组合，并专注于可用性。

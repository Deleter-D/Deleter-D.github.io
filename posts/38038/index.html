<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>CUDA编程——共享内存和常量内存 - 亦初</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="亦初"><meta name="msapplication-TileImage" content="https://user-images.githubusercontent.com/56388518/193990104-d040c2b4-1b96-4636-b410-b2ccd6360665.jpg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="亦初"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="很多人是参考《Professional CUDA C Programming》一书来入门CUDA的，这本书本身是很好的入门材料，但由于CUDA版本迭代非常快，导致书中的一些内容已经是过时的了。这也是笔者撰写本系列博客的初衷之一，这个系列参考了本书以及CUDA 12.x的官方文档，并在每个章节都附有详细的代码参考，并且代码是基于CUDA 12.x的，可以解决一些由于版本迭代带来的问题。本系列的博客由"><meta property="og:type" content="blog"><meta property="og:title" content="亦初"><meta property="og:url" content="https://deleter-d.github.io/"><meta property="og:site_name" content="亦初"><meta property="og:description" content="很多人是参考《Professional CUDA C Programming》一书来入门CUDA的，这本书本身是很好的入门材料，但由于CUDA版本迭代非常快，导致书中的一些内容已经是过时的了。这也是笔者撰写本系列博客的初衷之一，这个系列参考了本书以及CUDA 12.x的官方文档，并在每个章节都附有详细的代码参考，并且代码是基于CUDA 12.x的，可以解决一些由于版本迭代带来的问题。本系列的博客由"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://user-images.githubusercontent.com/56388518/193990104-d040c2b4-1b96-4636-b410-b2ccd6360665.jpg"><meta property="article:published_time" content="2024-02-20T08:22:30.000Z"><meta property="article:modified_time" content="2024-02-27T09:37:44.297Z"><meta property="article:author" content="亦初"><meta property="article:tag" content="异构计算"><meta property="article:tag" content="CUDA"><meta property="article:tag" content="高性能计算"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="https://user-images.githubusercontent.com/56388518/193990104-d040c2b4-1b96-4636-b410-b2ccd6360665.jpg"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://deleter-d.github.io/posts/38038/"},"headline":"亦初","image":[],"datePublished":"2024-02-20T08:22:30.000Z","dateModified":"2024-02-27T09:37:44.297Z","author":{"@type":"Person","name":"亦初"},"description":"很多人是参考《Professional CUDA C Programming》一书来入门CUDA的，这本书本身是很好的入门材料，但由于CUDA版本迭代非常快，导致书中的一些内容已经是过时的了。这也是笔者撰写本系列博客的初衷之一，这个系列参考了本书以及CUDA 12.x的官方文档，并在每个章节都附有详细的代码参考，并且代码是基于CUDA 12.x的，可以解决一些由于版本迭代带来的问题。本系列的博客由"}</script><link rel="canonical" href="https://deleter-d.github.io/posts/38038/"><link rel="icon" href="https://user-images.githubusercontent.com/56388518/193990104-d040c2b4-1b96-4636-b410-b2ccd6360665.jpg"><meta name="referrer" content="no-referrer-when-downgrade"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/font-awesome/5.12.0/css/all.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css"><link rel="stylesheet" href="https://fonts.loli.net/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/font-awesome/5.12.0/css/all.min.css"><link rel="stylesheet" href="https://fonts.loli.net/css?family=Ubuntu:400,600|Source+Code+Pro|Monda:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Microsoft YaHei:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&amp;amp;subset=latin,latin-ext|Inconsolata|Itim|Lobster.css"><script src="https://cdnjs.loli.net/ajax/libs/jquery/3.3.1/jquery.min.js"></script><script src="/js/globalUtils.js"></script><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdnjs.loli.net/ajax/libs/pace/1.0.2/pace.min.js"></script><meta name="generator" content="Hexo 5.4.2"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style><link rel="alternate" href="/atom.xml" title="亦初" type="application/atom+xml">
</head><body class="is-3-column has-navbar-fixed-top"><nav class="navbar navbar-main is-fixed-top"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="https://user-images.githubusercontent.com/56388518/193990104-d040c2b4-1b96-4636-b410-b2ccd6360665.jpg" alt="亦初" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">首页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/music">音乐</a><a class="navbar-item" href="/message">留言</a><a class="navbar-item" href="/self-talking">碎碎念</a><a class="navbar-item" href="/about">关于</a></div><div class="navbar-end"><a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a><a class="navbar-item" id="night-nav" title="Night Mode" href="javascript:;"><i class="fas fa-moon" id="night-icon"></i></a></div></div></div></nav><script type="text/javascript" src="/js/theme-setting.js"></script><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><!--!--><div class="card"><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><i class="far fa-calendar-plus"> </i>2024-02-20  <a class="commentCountImg" href="/posts/38038/#comment-container"><span class="display-none-class">786495b5caba091c763264f572fdbb0b</span><i class="far fa-comment-dots"></i> <span class="commentCount" id="786495b5caba091c763264f572fdbb0b">99+</span>  </a><span class="level-item"><i class="far fa-clock"> </i>1 小时  <i class="fas fa-pencil-alt"> </i>11.8 k</span><span class="level-item" id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv">0</span>次访问</span></div></div><h1 class="title is-3 is-size-4-mobile">CUDA编程——共享内存和常量内存</h1><div class="content"><p>很多人是参考《Professional CUDA C Programming》一书来入门CUDA的，这本书本身是很好的入门材料，但由于CUDA版本迭代非常快，导致书中的一些内容已经是过时的了。这也是笔者撰写本系列博客的初衷之一，这个系列参考了本书以及CUDA 12.x的官方文档，并在每个章节都附有详细的代码参考，并且代码是基于CUDA 12.x的，可以解决一些由于版本迭代带来的问题。本系列的博客由《Professional CUDA C Programming》一书、CUDA官方文档、互联网上的一些资料以及笔者自己的理解构成，希望能对你有一些帮助，若有错误也请大胆指出。</p>
<span id="more"></span>
<h2 id="共享内存概述">共享内存概述</h2>
<p>GPU中有两种类型的内存：</p>
<ul>
<li>板载内存：全局内存是较大的板载内存，延迟相对较高；</li>
<li>片上内存：共享内存是较小的片上内存，延迟相对较低，同时带宽比全局内存高得多。</li>
</ul>
<p>共享内存可以视作一个可编程的缓存，一般用于块内线程的通信，全局内存数据的可编程管理缓存，以及高速暂存存储器，用于转换数据以优化全局内存访问模式。</p>
<h3 id="共享内存">共享内存</h3>
<p>使用内存空间说明符<code>__shared__</code>分配共享内存（Shared Memory）。再次回顾下图中的内存层次结构。</p>
<p><img src="https://github.com/Deleter-D/Images/assets/56388518/78acbc6e-a8c2-4440-a03d-1d9453aaa3cb"></p>
<p>共享内存比全局内存快的多，可以当作暂存器或由程序管理的高速缓存来使用，以最小化block对全局内存的访问。</p>
<p>下面是直接实现矩阵乘法的例子，未使用共享内存，每个线程读取矩阵A的一行和矩阵B的一列进行计算。矩阵A将被从全局内存中读取<code>B.width</code>次，矩阵B将被从全局内存中读取<code>A.height</code>次。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-comment">// 矩阵为行优先存储</span><br><span class="hljs-comment">// M(row, col) = *(M.elements + row * M.width + col)</span><br><span class="hljs-keyword">typedef</span> <span class="hljs-keyword">struct</span> {<br>    <span class="hljs-type">int</span> width;<br>    <span class="hljs-type">int</span> height;<br>    <span class="hljs-type">float</span>* elements;<br>} Matrix;<br><br><span class="hljs-comment">// 定义block的大小</span><br><span class="hljs-meta">#<span class="hljs-keyword">define</span> BLOCK_SIZE 16</span><br><br><span class="hljs-comment">// 矩阵乘法核函数</span><br><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">MatMulKernel</span><span class="hljs-params">(Matrix A, Matrix B, Matrix C)</span></span><br><span class="hljs-function"></span>{<br>    <span class="hljs-comment">// 每个线程计算一个C的元素</span><br>    <span class="hljs-type">float</span> Cvalue = <span class="hljs-number">0</span>;<br>    <span class="hljs-type">int</span> row = blockIdx.y * blockDim.y + threadIdx.y;<br>    <span class="hljs-type">int</span> col = blockIdx.x * blockDim.x + threadIdx.x;<br>    <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> e = <span class="hljs-number">0</span>; e &lt; A.width; ++e)<br>        Cvalue += A.elements[row * A.width + e * B.elements[e * B.width + col];<br>    C.elements[row * C.width + col] = Cvalue;<br>}<br><br><span class="hljs-comment">// 主机代码</span><br><span class="hljs-comment">// 假设矩阵的维数能够被BLOCK_SIZE整除</span><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">MatMul</span><span class="hljs-params">(<span class="hljs-type">const</span> Matrix A, <span class="hljs-type">const</span> Matrix B, Matrix C)</span></span><br><span class="hljs-function"></span>{<br>    <span class="hljs-comment">// 将A和B加载到设备内存</span><br>    Matrix d_A;<br>    d_A.width = A.width;<br>    d_A.height = A.height;<br>    <span class="hljs-type">size_t</span> size = A.width * A.height * <span class="hljs-built_in">sizeof</span>(<span class="hljs-type">float</span>);<br>    <span class="hljs-built_in">cudaMalloc</span>(&amp;d_A.elements, size);<br>    <span class="hljs-built_in">cudaMemcpy</span>(d_A.elements, A.elements, size, cudaMemcpyHostToDevice);<br>    Matrix d_B;<br>    d_B.width = B.width;<br>    d_B.height = B.height;<br>    size = B.width * B.height * <span class="hljs-built_in">sizeof</span>(<span class="hljs-type">float</span>);<br>    <span class="hljs-built_in">cudaMalloc</span>(&amp;d_B.elements, size);<br>    <span class="hljs-built_in">cudaMemcpy</span>(d_B.elements, B.elements, size, cudaMemcpyHostToDevice);<br><br>    <span class="hljs-comment">// 在设备上申请结果矩阵</span><br>    Matrix d_C;<br>    d_C.width = C.width;<br>    d_C.height = C.height;<br>    size = C.width * C.height * <span class="hljs-built_in">sizeof</span>(<span class="hljs-type">float</span>);<br>    <span class="hljs-built_in">cudaMalloc</span>(&amp;d_C.elements, size);<br><br>    <span class="hljs-comment">// 调用核函数</span><br>    <span class="hljs-function">dim3 <span class="hljs-title">dimBlock</span><span class="hljs-params">(BLOCK_SIZE, BLOCK_SIZE)</span></span>;<br>    <span class="hljs-function">dim3 <span class="hljs-title">dimGrid</span><span class="hljs-params">(B.width / dimBlock.x, A.height / dimBlock.y)</span></span>;<br>    MatMulKernel&lt;&lt;&lt;dimGrid, dimBlock&gt;&gt;&gt;(d_A, d_B, d_C);<br><br>    <span class="hljs-comment">// 从设备内存中读取结果矩阵C</span><br>    <span class="hljs-built_in">cudaMemcpy</span>(C.elements, d_C.elements, size, cudaMemcpyDeviceToHost);<br><br>    <span class="hljs-comment">// 释放设备内存</span><br>    <span class="hljs-built_in">cudaFree</span>(d_A.elements);<br>    <span class="hljs-built_in">cudaFree</span>(d_B.elements);<br>    <span class="hljs-built_in">cudaFree</span>(d_C.elements);<br>}<br></code></pre></td></tr></table></figure>
<p>整个访存过程如下图所示。</p>
<p><img src="https://github.com/Deleter-D/Images/assets/56388518/9d695d44-1c15-4282-9d99-fedec7f36db6"></p>
<p>下面是使用共享内存实现矩阵乘法的例子。在下例中，每个block负责计算C的一个子矩阵<code>Csub</code>（方阵），block内的每个线程负责计算<code>Csub</code>的一个元素，<code>Csub</code>等于两个子矩阵的乘积。其中，A的子矩阵维度为<code>(A.width, block_size)</code>，行索引与<code>Csub</code>的行索引相同，B的子矩阵维度为<code>(block_size, A.width)</code>，列索引与<code>Csub</code>的列索引相同。</p>
<p>为了适配设备的资源，A、B的两个子矩阵被划分为多个维度为<code>block_size</code>的方阵，<code>Csub</code>即为这些方阵的乘积之和。将两个对应的方阵从全局内存中加载到共享内存中，其中每个线程加载两个对应方阵中的各一个元素，然后每个线程计算一个乘积。每个参与计算的线程都将乘积累加到一个寄存器中，完成累加后将结果写入全局内存。</p>
<p>通过这种方式，利用了共享内存的速度优势，节省了大量全局内存带宽。A只从全局内存中读取<code>(B.width / block_size)</code>次，而B只读取了<code>(A.height / block_size)</code>次。</p>
<p>下例还引入了stride，可以用相同的类型有效地表示子矩阵。利用了一个设备函数来获取和设置元素，并从矩阵中构建子矩阵。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-comment">// 矩阵为行优先存储</span><br><span class="hljs-comment">// M(row, col) = *(M.elements + row * M.stride + col)</span><br><span class="hljs-keyword">typedef</span> <span class="hljs-keyword">struct</span> {<br>    <span class="hljs-type">int</span> width;<br>    <span class="hljs-type">int</span> height;<br>    <span class="hljs-type">int</span> stride;<br>    <span class="hljs-type">float</span>* elements;<br>} Matrix;<br><br><span class="hljs-comment">// 获取一个矩阵元素</span><br><span class="hljs-function">__device__ <span class="hljs-type">float</span> <span class="hljs-title">GetElement</span><span class="hljs-params">(<span class="hljs-type">const</span> Matrix A, <span class="hljs-type">int</span> row, <span class="hljs-type">int</span> col)</span></span><br><span class="hljs-function"></span>{<br>    <span class="hljs-keyword">return</span> A.elements[row * A.stride + col];<br>}<br><span class="hljs-comment">// 设置一个矩阵元素</span><br><span class="hljs-function">__device__ <span class="hljs-type">void</span> <span class="hljs-title">SetElement</span><span class="hljs-params">(Matrix A, <span class="hljs-type">int</span> row, <span class="hljs-type">int</span> col, <span class="hljs-type">float</span> value)</span></span><br><span class="hljs-function"></span>{<br>    A.elements[row * A.stride + col] = value;<br>}<br><br><span class="hljs-comment">// 获取A的BLOCK_SIZE*BLOCK_SIZE子矩阵Asub，通过row、col以及stride和BLOCK_SIZE来定位</span><br> <span class="hljs-function">__device__ Matrix <span class="hljs-title">GetSubMatrix</span><span class="hljs-params">(Matrix A, <span class="hljs-type">int</span> row, <span class="hljs-type">int</span> col)</span></span><br><span class="hljs-function"></span>{<br>    Matrix Asub;<br>    Asub.width    = BLOCK_SIZE;<br>    Asub.height   = BLOCK_SIZE;<br>    Asub.stride   = A.stride;<br>    Asub.elements = &amp;A.elements[A.stride * BLOCK_SIZE * row + BLOCK_SIZE * col];<br>    <span class="hljs-keyword">return</span> Asub;<br>}<br><br><span class="hljs-comment">// 定义block的大小</span><br><span class="hljs-meta">#<span class="hljs-keyword">define</span> BLOCK_SIZE 16</span><br><br><span class="hljs-comment">// 矩阵乘法核函数的前置声明</span><br><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">MatMulKernel</span><span class="hljs-params">(<span class="hljs-type">const</span> Matrix, <span class="hljs-type">const</span> Matrix, Matrix)</span></span>;<br><br><span class="hljs-comment">// 矩阵乘法核函数</span><br> <span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">MatMulKernel</span><span class="hljs-params">(Matrix A, Matrix B, Matrix C)</span></span><br><span class="hljs-function"></span>{<br>    <span class="hljs-comment">// block的行和列</span><br>    <span class="hljs-type">int</span> blockRow = blockIdx.y;<br>    <span class="hljs-type">int</span> blockCol = blockIdx.x;<br>    <span class="hljs-comment">// 每个block计算的子矩阵Csub</span><br>    Matrix Csub = <span class="hljs-built_in">GetSubMatrix</span>(C, blockRow, blockCol);<br>    <span class="hljs-comment">// 每个线程计算Csub的一个元素</span><br>    <span class="hljs-type">float</span> Cvalue = <span class="hljs-number">0</span>;<br>    <span class="hljs-comment">// Csub中的线程的行和列</span><br>    <span class="hljs-type">int</span> row = threadIdx.y;<br>    <span class="hljs-type">int</span> col = threadIdx.x;<br>    <span class="hljs-comment">// 循环计算Csub所需的A和B的所有子矩阵，将每对子矩阵相乘并累加结果</span><br>    <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> m = <span class="hljs-number">0</span>; m &lt; (A.width / BLOCK_SIZE); ++m) {<br>        <span class="hljs-comment">// 获取A的子矩阵Asub</span><br>        Matrix Asub = <span class="hljs-built_in">GetSubMatrix</span>(A, blockRow, m);<br>        <span class="hljs-comment">// 获取B的子矩阵Bsub</span><br>        Matrix Bsub = <span class="hljs-built_in">GetSubMatrix</span>(B, m, blockCol);<br>        <span class="hljs-comment">// 用于分别存储Asub和Bsub的共享内存</span><br>        __shared__ <span class="hljs-type">float</span> As[BLOCK_SIZE][BLOCK_SIZE];<br>        __shared__ <span class="hljs-type">float</span> Bs[BLOCK_SIZE][BLOCK_SIZE];<br>        <span class="hljs-comment">// 将Asub和Bsub从设备内存加载到共享内存</span><br>        <span class="hljs-comment">// 每个线程加载每个子矩阵的一个元素</span><br>        As[row][col] = <span class="hljs-built_in">GetElement</span>(Asub, row, col);<br>        Bs[row][col] = <span class="hljs-built_in">GetElement</span>(Bsub, row, col);<br>        <span class="hljs-comment">// 同步，确保在开始计算之前子矩阵加载完整</span><br>        __syncthreads();<br>        <span class="hljs-comment">// 将Asub和Bsub相乘</span><br>        <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> e = <span class="hljs-number">0</span>; e &lt; BLOCK_SIZE; ++e)<br>            Cvalue += As[row][e] * Bs[e][col];<br>        <span class="hljs-comment">// 同步，确保下一次循环加载A和B的两个新子矩阵之前完成之前的计算</span><br>        __syncthreads();<br>    }<br>    <span class="hljs-comment">// 将Csub写入设备内存，每个线程写入一个元素</span><br>    <span class="hljs-built_in">SetElement</span>(Csub, row, col, Cvalue);<br>}<br><br><span class="hljs-comment">// 主机代码基本一致，此处省略</span><br></code></pre></td></tr></table></figure>
<p>整个访存过程如下图所示。</p>
<p><img src="https://github.com/Deleter-D/Images/assets/56388518/3e53d361-1acc-4446-ac80-221577541d6a"></p>
<blockquote>
<p>物理上，每个SM都有一个小的低延迟内存池，该内存池被当前正在该SM上执行的线程块中的所有线程共享。</p>
<p>当每个线程块开始工作时，会分配给它一定数量的共享内存。它的内容和创建时所在的线程块具有相同的生命周期。每个线程束发出的共享内存访问请求，理想情况下，每个请求在一个事务中完成。最坏情况下，每个共享内存的请求在32个不同的事务中顺序执行。若多个线程访问共享内存中的同一个字，一个线程读取该字后，会通过多播把它发给其他线程。</p>
<p>共享内存被SM中所有常驻线程块划分，因此共享内存是限制设备并行性的关键资源。一个核函数使用的共享内存越多，处于并发活跃状态的线程块就越少。</p>
</blockquote>
<h3 id="分布式共享内存">分布式共享内存</h3>
<p>在计算能力9.0中引入的线程块集群为线程块集群中的线程提供了访问集群中所有参与线程块的共享内存能力。属于线程块集群的线程可以在分布式地址空间中读取、写入或执行原子，无论目标地址属于当前线程块还是集群中的其他线程块。无论核函数是否使用分布式共享内存（Distributed Shared Memory），共享内存范围仍然属于各自的线程块。分布式共享内存的大小就是每个集群的线程块数乘以每个线程块的共享内存大小。</p>
<p>访问分布式共享内存中的数据需要所有线程块都存在。使用<code>cluster Group</code>API中的<code>cluster.sync()</code>可以保证所有线程块都已开始执行。还需要确保所有分布式共享内存操作都发生在线程块退出之前。</p>
<p>下面是一个计算直方图的例子，以及如何使用线程块集群在GPU上优化计算。计算直方图的一个标准方法是在每个线程块的共享内存中进行计算，然后在全局内存中执行原子操作。这种方法的一个限制是共享内存的容量，如果直方图的数据量无法与共享内存适配，就只能在全局内存中直接进行原子操作。</p>
<p>而对于分布式共享内存，CUDA提供了一个中间步骤，可以根据直方图的数据量，选择在共享内存、分布式共享内存或全局内存中计算直方图。</p>
<p>下面的CUDA核函数实现了根据直方图数据量选择计算直方图的内存空间。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta">#<span class="hljs-keyword">include</span> <span class="hljs-string">&lt;cooperative_groups.h&gt;</span></span><br><br><span class="hljs-comment">// 分布式共享内存计算直方图核函数</span><br><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">clusterHist_kernel</span><span class="hljs-params">(<span class="hljs-type">int</span> *bins, <span class="hljs-type">const</span> <span class="hljs-type">int</span> nbins, <span class="hljs-type">const</span> <span class="hljs-type">int</span> bins_per_block, </span></span><br><span class="hljs-params"><span class="hljs-function">                                   <span class="hljs-type">const</span> <span class="hljs-type">int</span> *__restrict__ input, <span class="hljs-type">size_t</span> array_size)</span></span><br><span class="hljs-function"></span>{<br>  <span class="hljs-keyword">extern</span> __shared__ <span class="hljs-type">int</span> smem[];<br>  <span class="hljs-keyword">namespace</span> cg = cooperative_groups;<br>  <span class="hljs-type">int</span> tid = cg::<span class="hljs-built_in">this_grid</span>().<span class="hljs-built_in">thread_rank</span>();<br><br>  <span class="hljs-comment">// 集群初始化，获取集群尺寸，并计算局部数据偏移</span><br>  cg::cluster_group cluster = cg::<span class="hljs-built_in">this_cluster</span>();<br>  <span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> clusterBlockRank = cluster.<span class="hljs-built_in">block_rank</span>();<br>  <span class="hljs-type">int</span> cluster_size = cluster.<span class="hljs-built_in">dim_blocks</span>().x;<br><br>  <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = threadIdx.x; i &lt; bins_per_block; i += blockDim.x)<br>  {<br>    smem[i] = <span class="hljs-number">0</span>; <span class="hljs-comment">//将共享内存中的直方图初始化为0</span><br>  }<br><br>  <span class="hljs-comment">// 集群同步，确保集群中的所有block的共享内存初始化为0，并确保所有block都开始执行且同时存在</span><br>  cluster.<span class="hljs-built_in">sync</span>();<br><br>  <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = tid; i &lt; array_size; i += blockDim.x * gridDim.x)<br>  {<br>    <span class="hljs-type">int</span> ldata = input[i];<br><br>    <span class="hljs-comment">// 寻找正确的直方图数据</span><br>    <span class="hljs-type">int</span> binid = ldata;<br>    <span class="hljs-keyword">if</span> (ldata &lt; <span class="hljs-number">0</span>)<br>      binid = <span class="hljs-number">0</span>;<br>    <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (ldata &gt;= nbins)<br>      binid = nbins - <span class="hljs-number">1</span>;<br><br>    <span class="hljs-comment">// 寻找目标block的行索引和行内偏移，以计算分布式共享内存的直方图</span><br>    <span class="hljs-type">int</span> dst_block_rank = (<span class="hljs-type">int</span>)(binid / bins_per_block);<br>    <span class="hljs-type">int</span> dst_offset = binid % bins_per_block;<br><br>    <span class="hljs-comment">// 指向目标block共享内存的指针</span><br>    <span class="hljs-type">int</span> *dst_smem = cluster.<span class="hljs-built_in">map_shared_rank</span>(smem, dst_block_rank);<br><br>    <span class="hljs-comment">// 执行原子更新操作</span><br>    <span class="hljs-built_in">atomicAdd</span>(dst_smem + dst_offset, <span class="hljs-number">1</span>);<br>  }<br><br>  <span class="hljs-comment">// 这里的集群同步是必须的，以确保所有分布式共享内存的操作都已完成</span><br>  <span class="hljs-comment">// 并确保当某个block在访问分布式共享内存时，不会有其他block结束。</span><br>  cluster.<span class="hljs-built_in">sync</span>();<br><br>  <span class="hljs-comment">// 使用局部分布式内存中的直方图，计算全局内存的直方图</span><br>  <span class="hljs-type">int</span> *lbins = bins + cluster.<span class="hljs-built_in">block_rank</span>() * bins_per_block;<br>  <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = threadIdx.x; i &lt; bins_per_block; i += blockDim.x)<br>  {<br>    <span class="hljs-built_in">atomicAdd</span>(&amp;lbins[i], smem[i]);<br>  }<br>}<br></code></pre></td></tr></table></figure>
<p>上面的核函数可以在运行时指定集群大小，集群的大小取决于分布式共享内存的需求。若直方图足够小，只能填满一个block的共享内存，则可以启动集群大小为1的核函数。</p>
<p>下面的代码实现了根据共享内存需求动态确定集群大小。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-comment">// Launch via extensible launch</span><br><span class="hljs-comment">// </span><br>{<br>  cudaLaunchConfig_t config = {<span class="hljs-number">0</span>};<br>  config.gridDim = array_size / threads_per_block;<br>  config.blockDim = threads_per_block;<br><br>  <span class="hljs-comment">// 集群大小取决于直方图的大小</span><br>  <span class="hljs-comment">// cluster_size == 1意味着不会有分布式共享内存，只有block局部共享内存</span><br>  <span class="hljs-type">int</span> cluster_size = <span class="hljs-number">2</span>; <span class="hljs-comment">// 以集群大小2为例</span><br>  <span class="hljs-type">int</span> nbins_per_block = nbins / cluster_size; <span class="hljs-comment">// 每个block的动态共享内存大小</span><br>  <br>  <span class="hljs-comment">// 分布式共享内存大小为cluster_size * nbins_per_block * sizeof(int)</span><br>  config.dynamicSmemBytes = nbins_per_block * <span class="hljs-built_in">sizeof</span>(<span class="hljs-type">int</span>);<br><br>  <span class="hljs-built_in">CUDA_CHECK</span>(::<span class="hljs-built_in">cudaFuncSetAttribute</span>((<span class="hljs-type">void</span> *)clusterHist_kernel, <br>                                    cudaFuncAttributeMaxDynamicSharedMemorySize,<br>                                    config.dynamicSmemBytes));<br><br>  cudaLaunchAttribute attribute[<span class="hljs-number">1</span>];<br>  attribute[<span class="hljs-number">0</span>].id = cudaLaunchAttributeClusterDimension;<br>  attribute[<span class="hljs-number">0</span>].val.clusterDim.x = cluster_size;<br>  attribute[<span class="hljs-number">0</span>].val.clusterDim.y = <span class="hljs-number">1</span>;<br>  attribute[<span class="hljs-number">0</span>].val.clusterDim.z = <span class="hljs-number">1</span>;<br><br>  config.numAttrs = <span class="hljs-number">1</span>;<br>  config.attrs = attribute;<br><br>  <span class="hljs-built_in">cudaLaunchKernelEx</span>(&amp;config, clusterHist_kernel, bins, nbins, nbins_per_block, input, array_size);<br>}<br></code></pre></td></tr></table></figure>
<h3 id="共享内存的分配">共享内存的分配</h3>
<p>共享内存可以被静态或动态分配。上面也提到过，使用<code>__shared__</code>修饰符来申请共享内存。</p>
<p>下面的代码申请了一个共享内存中的二维数组。若该声明在核函数内，则该变量的作用域为该核函数中。若在文件的任何核函数外声明，则作用域对所有核函数来说是全局的。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cpp">__shared__ <span class="hljs-type">float</span> tile[size_y][size_x];<br></code></pre></td></tr></table></figure>
<p>若共享内存的大小在编译时无法确定，则可以用<code>extern</code>关键字进行声明。该声明同样可以在核函数内或核函数外。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-keyword">extern</span> __shared__ <span class="hljs-type">int</span> tile[];<br></code></pre></td></tr></table></figure>
<p>由于该数组的大小编译时是未知的，所以在核函数被调用时，需要动态分配共享内存。将所需的共享内存字节数在<code>&lt;&lt;&lt;...&gt;&gt;&gt;</code>的第三个参数传入。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cpp">kernel&lt;&lt;&lt;grid, block, <span class="hljs-function">size * <span class="hljs-title">sizeof</span><span class="hljs-params">(<span class="hljs-type">int</span>)</span>&gt;&gt;&gt;<span class="hljs-params">(...)</span></span><br></code></pre></td></tr></table></figure>
<p>注意：只能动态声明一维数组。</p>
<h3 id="共享内存存储体和访问模式">共享内存存储体和访问模式</h3>
<h4 id="内存存储体">内存存储体</h4>
<p>为了获取高内存带宽，共享内存被分为32个同样大小的内存模型，被成为存储体，它们可以被同时访问。共享内存是一个一维地址空间。根据GPU的计算能力，共享内存的地址在不同模式下会映射到不同的存储体中。</p>
<p>如果通过线程束发布共享内存加载或存储操作，且在每个存储体上只访问不多于一个的内存地址，则该操作可以由一个内存事务来完成。否则该操作由多个内存事务完成，这样就降低了内存带宽的利用率。</p>
<h4 id="存储体冲突">存储体冲突</h4>
<p>在共享内存中，多个地址请求落在同一个存储体时，就会发生存储体冲突，这会导致请求被重复执行。硬件会将存储体冲突的请求分割到尽可能多的独立无冲突事务中。当线程束发出共享内存请求时，有3种典型模式：</p>
<ul>
<li>并行访问：多个地址访问多个存储体；</li>
<li>串行访问：多个地址访问同一个存储体；</li>
<li>广播访问：单一地址读取单一存储体。</li>
</ul>
<p>并行访问是最常见的模式，这种模式下如果访问的不是范围内的所有地址，则至少有一些地址可以在一个内存事务中完成。最佳情况是每个地址都位于一个单独的存储体中，执行无冲突的共享内存访问。</p>
<p>串行访问是最坏的模式。如果线程束中的32个线程全都访问同一存储体中的不同地址，则需要32个内存事务，且是串行执行的。</p>
<p>广播访问的情况下，线程束中的所有线程都读取同一存储体的同一地址。若一个内存事务被执行，则被访问的字会广播到所有请求线程中。虽然只需要一个内存事务，但因为只有一小部分字节被读取，所以带宽利用率很差。</p>
<h4 id="访问模式">访问模式</h4>
<p>内存存储体的宽度和设备计算能力有关，根据官方文档的描述，总结如下：</p>
<ul>
<li>计算能力2.x、5.x、6.x、7.x、8.x存储体数量均为32个，宽度均为32bit；</li>
<li>计算能力3.x比较特殊，存储体数量为32个，宽度64bit。</li>
</ul>
<p>对于32个32位的存储体来说，每个存储体在每两个时钟周期内都有32位的带宽，连续的32位字映射到连续的存储体中，因此共享内存地址到存储体索引的映射可以由下列公式计算。 <span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="33.183ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 14667 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">存</text></g><g data-mml-node="mi" transform="translate(1000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">储</text></g><g data-mml-node="mi" transform="translate(2000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">体</text></g><g data-mml-node="mi" transform="translate(3000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">索</text></g><g data-mml-node="mi" transform="translate(4000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">引</text></g><g data-mml-node="mo" transform="translate(5277.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mo" transform="translate(6333.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(6722.6,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">字</text></g><g data-mml-node="mi" transform="translate(7722.6,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">节</text></g><g data-mml-node="mi" transform="translate(8722.6,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">地</text></g><g data-mml-node="mi" transform="translate(9722.6,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">址</text></g><g data-mml-node="mo" transform="translate(10944.8,0)"><path data-c="F7" d="M318 466Q318 500 339 518T386 537Q418 537 438 517T458 466Q458 438 440 417T388 396Q355 396 337 417T318 466ZM56 237T56 250T70 270H706Q721 262 721 250T706 230H70Q56 237 56 250ZM318 34Q318 68 339 86T386 105Q418 105 438 85T458 34Q458 6 440 -15T388 -36Q355 -36 337 -15T318 34Z"></path></g><g data-mml-node="mn" transform="translate(11945,0)"><path data-c="34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z"></path></g><g data-mml-node="mo" transform="translate(12445,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mi" transform="translate(12834,0)"><path data-c="25" d="M465 605Q428 605 394 614T340 632T319 641Q332 608 332 548Q332 458 293 403T202 347Q145 347 101 402T56 548Q56 637 101 693T202 750Q241 750 272 719Q359 642 464 642Q580 642 650 732Q662 748 668 749Q670 750 673 750Q682 750 688 743T693 726Q178 -47 170 -52Q166 -56 160 -56Q147 -56 142 -45Q137 -36 142 -27Q143 -24 363 304Q469 462 525 546T581 630Q528 605 465 605ZM207 385Q235 385 263 427T292 548Q292 617 267 664T200 712Q193 712 186 709T167 698T147 668T134 615Q132 595 132 548V527Q132 436 165 403Q183 385 203 385H207ZM500 146Q500 234 544 290T647 347Q699 347 737 292T776 146T737 0T646 -56Q590 -56 545 0T500 146ZM651 -18Q679 -18 707 24T736 146Q736 215 711 262T644 309Q637 309 630 306T611 295T591 265T578 212Q577 200 577 146V124Q577 -18 647 -18H651Z"></path></g><g data-mml-node="mn" transform="translate(13667,0)"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" transform="translate(500,0)"></path></g></g></g></svg></mjx-container></span> 映射关系如下图所示。</p>
<p><img src="https://github.com/Deleter-D/Images/assets/56388518/58dfa384-6b77-41f3-bb32-5def21815fbc"></p>
<p>对于计算能力3.x的设备，存储体宽度为64bit，但它有两种地址模式，64位模式与32位模式。</p>
<p>在64模式下，每个存储体在每两个时钟周期内都有64位的带宽，连续的64位字映射到连续的存储体中，因此共享内存地址到存储体索引的映射可以由下列公式计算。 <span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="33.183ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 14667 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">存</text></g><g data-mml-node="mi" transform="translate(1000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">储</text></g><g data-mml-node="mi" transform="translate(2000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">体</text></g><g data-mml-node="mi" transform="translate(3000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">索</text></g><g data-mml-node="mi" transform="translate(4000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">引</text></g><g data-mml-node="mo" transform="translate(5277.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mo" transform="translate(6333.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(6722.6,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">字</text></g><g data-mml-node="mi" transform="translate(7722.6,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">节</text></g><g data-mml-node="mi" transform="translate(8722.6,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">地</text></g><g data-mml-node="mi" transform="translate(9722.6,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">址</text></g><g data-mml-node="mo" transform="translate(10944.8,0)"><path data-c="F7" d="M318 466Q318 500 339 518T386 537Q418 537 438 517T458 466Q458 438 440 417T388 396Q355 396 337 417T318 466ZM56 237T56 250T70 270H706Q721 262 721 250T706 230H70Q56 237 56 250ZM318 34Q318 68 339 86T386 105Q418 105 438 85T458 34Q458 6 440 -15T388 -36Q355 -36 337 -15T318 34Z"></path></g><g data-mml-node="mn" transform="translate(11945,0)"><path data-c="38" d="M70 417T70 494T124 618T248 666Q319 666 374 624T429 515Q429 485 418 459T392 417T361 389T335 371T324 363L338 354Q352 344 366 334T382 323Q457 264 457 174Q457 95 399 37T249 -22Q159 -22 101 29T43 155Q43 263 172 335L154 348Q133 361 127 368Q70 417 70 494ZM286 386L292 390Q298 394 301 396T311 403T323 413T334 425T345 438T355 454T364 471T369 491T371 513Q371 556 342 586T275 624Q268 625 242 625Q201 625 165 599T128 534Q128 511 141 492T167 463T217 431Q224 426 228 424L286 386ZM250 21Q308 21 350 55T392 137Q392 154 387 169T375 194T353 216T330 234T301 253T274 270Q260 279 244 289T218 306L210 311Q204 311 181 294T133 239T107 157Q107 98 150 60T250 21Z"></path></g><g data-mml-node="mo" transform="translate(12445,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mi" transform="translate(12834,0)"><path data-c="25" d="M465 605Q428 605 394 614T340 632T319 641Q332 608 332 548Q332 458 293 403T202 347Q145 347 101 402T56 548Q56 637 101 693T202 750Q241 750 272 719Q359 642 464 642Q580 642 650 732Q662 748 668 749Q670 750 673 750Q682 750 688 743T693 726Q178 -47 170 -52Q166 -56 160 -56Q147 -56 142 -45Q137 -36 142 -27Q143 -24 363 304Q469 462 525 546T581 630Q528 605 465 605ZM207 385Q235 385 263 427T292 548Q292 617 267 664T200 712Q193 712 186 709T167 698T147 668T134 615Q132 595 132 548V527Q132 436 165 403Q183 385 203 385H207ZM500 146Q500 234 544 290T647 347Q699 347 737 292T776 146T737 0T646 -56Q590 -56 545 0T500 146ZM651 -18Q679 -18 707 24T736 146Q736 215 711 262T644 309Q637 309 630 306T611 295T591 265T578 212Q577 200 577 146V124Q577 -18 647 -18H651Z"></path></g><g data-mml-node="mn" transform="translate(13667,0)"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" transform="translate(500,0)"></path></g></g></g></svg></mjx-container></span> 在32位模式下，在同一存储体中访问两个32位字不一定是重复操作。在一个时钟周期内读64位并只将32位请求传输给线程是允许的。由于64位宽度的存储体比较少见，这里不过多阐述。</p>
<h4 id="内存填充">内存填充</h4>
<p>内存填充是避免存储体冲突的一种方法。以下图为例，假设只有5个存储体，若所有线程都访问bank0的不同地址，则会发生存储体冲突。但在每N个字后添加一个字，会使得原先同在bank0中的字改变位置，从而解决这种冲突。这里的N是存储体的数量。</p>
<p><img src="https://github.com/Deleter-D/Images/assets/56388518/799235c7-5a17-467e-9eb2-50c6e449dced"></p>
<h3 id="配置共享内存容量">配置共享内存容量</h3>
<p>每个SM都有64KB的片上内存，共享内存和一级缓存共享硬件资源。CUDA为配置一级缓存和共享内存容量提供了两种方法：</p>
<ul>
<li>按设备进行配置；</li>
<li>按核函数进行配置。</li>
</ul>
<p>使用<code>cudaDeviceSetCacheConfig()</code>运行时API可以在设备层面设置一级缓存和共享内存大小，可选参数与下面的运行时API一致。</p>
<p>使用<code>cudaFuncSetCacheConfig()</code>运行时API可以在核函数层面设置，相关参数已经在<a href="https://deleter-d.github.io/posts/47184/#共享内存">共享内存</a>阐述过了。</p>
<h3 id="同步">同步</h3>
<p>CUDA提供的块内同步有两个基本方法：</p>
<ul>
<li>障碍：所有调用的线程等待其余调用的线程达到障碍点；</li>
<li>内存栅栏：所有调用的线程必须等到全部内存修改对其余调用线程可见。</li>
</ul>
<h4 id="弱排序内存模型">弱排序内存模型</h4>
<p>GPU线程在不同内存中写入数据的顺序，不一定和这些数据在源代码中访问的顺序相同。一个线程的写入顺序对其他线程可见时，它可能和写操作被执行的实际顺序不一致。</p>
<p>若指令间是相互独立的，线程从不同内存中读取数据的顺序和读指令在程序中出现的顺序不一定相同。</p>
<h4 id="显式障碍">显式障碍</h4>
<p>设置障碍点的方法在前面已经见过了，即<code>__syncthreads()</code>函数。它要求块内的线程必须等待直到所有线程都到达该点。<code>__syncthreads()</code>还确保在障碍点之前，被这些线程访问的所有全局和共享内存对同一块中的所有线程可见。<code>__syncthreads()</code>用于协调同一块中线程间的通信。</p>
<p>在使用<code>__syncthreads()</code>时，必须保证一个条件能对整个线程块中的线程进行评估，否则执行很可能挂起甚至产生意料之外的问题。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-keyword">if</span> (threadID % <span class="hljs-number">2</span> == <span class="hljs-number">0</span>) {<br>    __syncthreads();<br>} <span class="hljs-keyword">else</span> {<br>    __syncthreads();<br>}<br></code></pre></td></tr></table></figure>
<p>上面的例子中，可能会导致线程无限期的等待对方，因为块中的所有线程没有到达相同的障碍点。</p>
<p>如果需要进行块间同步，可以尝试在同步点分割核函数并执行多个核函数启动，其中会产生隐式全局障碍以达到预期效果。</p>
<h4 id="内存栅栏">内存栅栏</h4>
<p>CUDA提供3种内存栅栏：</p>
<ul>
<li>块：<code>__threadfence_block()</code>；</li>
<li>网格：<code>__threadfence()</code>；</li>
<li>系统：<code>__threadfence_system()</code>。</li>
</ul>
<p><code>__threadfence_block()</code>保证了栅栏前被调用线程产生的对共享内存和全局内存的所有写操作对栅栏后同一块中的其他线程可见。内存栅栏不执行任何线程同步，所以对于一个块中的所有线程来说，没必要实际执行这个指令。</p>
<p><code>__threadfence()</code>会挂起调用的线程，直到全局内存中的所有写操作对同一网格内的所有线程均可见。</p>
<p><code>__threadfence_system()</code>会挂起调用的线程，以确保该线程对全局内存、锁页主机内存和其他设备内存中的所有写操作对全部设备中的线程和主机线程都可见。</p>
<h4 id="内存同步域">内存同步域</h4>
<h5 id="内存栅栏实例">内存栅栏实例</h5>
<p>某些CUDA程序可能会因为内存栅栏/刷新操作等待的事务多于CUDA内存一致性模型所需的事务而导致性能下降。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs c++">__managed__ <span class="hljs-type">int</span> x = <span class="hljs-number">0</span>;<br><span class="hljs-function">__device__ cuda::atomic&lt;<span class="hljs-type">int</span>, cuda::thread_scope_device&gt; <span class="hljs-title">a</span><span class="hljs-params">(<span class="hljs-number">0</span>)</span></span>;<br><span class="hljs-function">__managed__ cuda::atomic&lt;<span class="hljs-type">int</span>, cuda::thread_scope_system&gt; <span class="hljs-title">b</span><span class="hljs-params">(<span class="hljs-number">0</span>)</span></span>;<br></code></pre></td></tr></table></figure>
<p>Thread 1 (SM)：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs c++">x = <span class="hljs-number">1</span>;<br>a = <span class="hljs-number">1</span>;<br></code></pre></td></tr></table></figure>
<p>Thread 2 (SM)：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">while</span>(a != <span class="hljs-number">1</span>);<br><span class="hljs-built_in">assert</span>(x == <span class="hljs-number">1</span>);<br>b = <span class="hljs-number">1</span>;<br></code></pre></td></tr></table></figure>
<p>Thread 3 (CPU)：</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">while</span>(b != <span class="hljs-number">1</span>);<br><span class="hljs-built_in">assert</span>(x == <span class="hljs-number">1</span>);<br></code></pre></td></tr></table></figure>
<p>考虑上述例子，CUDA内存一致性模型将保证断言的条件为真，故在线程2写入<code>b</code>之前，线程1对<code>x</code>的写入必须对线程3可见。</p>
<p>释放和获取<code>a</code>所提供的内存排序仅能够使<code>x</code>对线程2可见，但线程3不可见，因为它是设备范围的操作。故由释放和获取<code>b</code>所提供的系统范围的内存排序需要确保从线程2发起的写入对线程3可见，同时要保证从线程2可见的其他线程发起的写入也是可见的。这要求称为累积性（cumulativity）。由于GPU在执行时不知道哪些写入在源码级别下是可见的，也不知道哪些写入仅在偶然的时间是可见的，所以它必须为所有活动状态的内存操作撒下一个保守的广域网。</p>
<p>上述情况会使GPU等待源码级别不需要的内存操作，进而使得内存栅栏/刷新操作花费不必要的时间，对整个程序产生干扰。</p>
<p>注意，内存栅栏可以在代码中显式的作为内部结构或原子操作出现，也可以隐式的实现任务边界上的同步关系。</p>
<h5 id="用域隔离流量">用域隔离流量</h5>
<p>从Hopper架构的GPU和CUDA 12.0开始，内存同步域提供了一种减轻上述干扰的方法。使用代码来显式辅助，可以减少GPU的光撒网行为。每次核函数启动都会被赋予一个域ID。写操作和栅栏都用该ID来标识，而栅栏只会对与栅栏域匹配的写操作进行排序。通信的核函数可以放在不同的域中。</p>
<p>使用域时，代码必须使同一GPU上不同域之间的排序或同步需要系统范围的栅栏。而在同一域中，设备范围的栅栏就足够了。这种栅栏范围要求对于累积性来说是必要的，因为一个核函数的写操作不会被另一个域中的核函数发出的栅栏所包围。本质上，累积性是通过确保跨域流量提前刷新到系统范围来满足的。</p>
<p>注意，这会修改<code>thread_scope_device</code>的定义。但由于核函数将默认采用域0，所以保证了向后兼容。</p>
<h5 id="在cuda中使用域">在CUDA中使用域</h5>
<p>域可以通过新的启动属性<code>cudaLaunchAttributeMemSyncDomain</code>和<code>cudaLaunchAttributeMemSyncDomainMap</code>来访问。前者在逻辑域<code>cudaLaunchMemSyncDomainDefault</code>和<code>cudaLaunchMemSyncDomainRemote</code>之间选择，后者提供从逻辑域到物理域的映射。远程域可供核函数进行远程内存访问，以便将其内存流量与本地核函数隔离。选择特定的域不会影响核函数可以合法执行的内存访问。</p>
<p>可以通过设备属性<code>cudaDevAttrMemSyncDomainCount</code>来获取域的数量。Hopper有4个域。为了便于代码移植，域功能可以在所有设备上使用，在Hopper架构之前的架构下，该计数将返回1。</p>
<p>详细参考官方文档<a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#using-domains-in-cuda">Using Domains in CUDA</a>。</p>
<h4 id="volatile修饰符"><code>volatile</code>修饰符</h4>
<p>在全局或共享内存中使用<code>volatile</code>修饰符声明一个变量，可以防止编译器优化，编译器优化可能会将数据暂时缓存在寄存器或本地内存中。当使用<code>volatile</code>修饰符时，编译器假定任何其他线程在任何时间都可以更改或使用该变量的值。因此，这个变量的任何引用都会直接被编译到全局内存读指令或全局内存写指令中，它们都会忽略缓存。</p>
<h2 id="共享内存的数据布局">共享内存的数据布局</h2>
<h3 id="方形共享内存">方形共享内存</h3>
<p>方形矩阵可以很容易从二维线程索引计算出一维内存偏移，声明一个方形二维共享内存变量。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cpp">__shared__ <span class="hljs-type">int</span> tile[N][N];<br></code></pre></td></tr></table></figure>
<p>它可以有两种访问方式：</p>
<ul>
<li><code>tile[threadIdx.y][threadIdx.x]</code></li>
<li><code>tile[threadIdx.x][threadIdx.y]</code></li>
</ul>
<p>显然，两种方式相比，第一种方式拥有更少的存储体冲突，因为邻近线程在最内层数组维度上访问相邻的阵列单元。</p>
<h4 id="访问方式实例">访问方式实例</h4>
<p>我们分别实现三种读写共享内存的方法：</p>
<ul>
<li>按行写入，按行读取；</li>
<li>按列写入，按列读取；</li>
<li>按列写入，按行读取。</li>
<li>按行写入，按列读取。</li>
</ul>
<blockquote>
<p>详细代码参考<a target="_blank" rel="noopener" href="https://github.com/Deleter-D/CUDA/blob/master/04_shared_and_constant_memory/01_square_shared_memory.cu">square_shared_memory.cu</a>。</p>
</blockquote>
<p>利用<code>nsys</code>来分析其耗时，并利用<code>ncu</code>来分析它们的共享内存加载和存储事务来体现存储体冲突，结果如下。</p>
<figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs scss">                                                  耗时	 加载事务  存储事务<br><span class="hljs-built_in">writeRowReadRow</span>(int *) (<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<span class="hljs-built_in">x</span>(<span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">1</span>)	<span class="hljs-number">1248</span> ns	  	  <span class="hljs-number">32</span>	  <span class="hljs-number">32</span><br><span class="hljs-built_in">writeColReadCol</span>(int *) (<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<span class="hljs-built_in">x</span>(<span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">1</span>)	<span class="hljs-number">1920</span> ns		<span class="hljs-number">1024</span>	<span class="hljs-number">1024</span><br><span class="hljs-built_in">writeColReadRow</span>(int *) (<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<span class="hljs-built_in">x</span>(<span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">1</span>)	<span class="hljs-number">1280</span> ns		  <span class="hljs-number">32</span>	<span class="hljs-number">1024</span><br><span class="hljs-built_in">writeRowReadCol</span>(int *) (<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<span class="hljs-built_in">x</span>(<span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">1</span>)	<span class="hljs-number">1280</span> ns		<span class="hljs-number">1024</span>	  <span class="hljs-number">32</span><br></code></pre></td></tr></table></figure>
<p>可以看出行读行写的核函数性能最高，加载和存储事务最少，没有存储体冲突。不管是按列读还是写，都会存在存储体冲突，导致加载或存储事务大量增多。</p>
<h4 id="动态共享内存">动态共享内存</h4>
<p>使用动态声明共享内存的方式实现与上述功能相同的核函数。动态共享内存必须被声明为一个未定大小的一维数组，因此需要基于二维线程索引来计算内存访问索引。</p>
<p>再测试其性能，结果如下。</p>
<figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs scss">                                                  		  耗时 	 加载事务  存储事务<br><span class="hljs-built_in">writeRowReadColDynamic</span>(int *) (<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<span class="hljs-built_in">x</span>(<span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">1</span>)		<span class="hljs-number">1280</span> ns		<span class="hljs-number">1024</span>	  <span class="hljs-number">32</span><br></code></pre></td></tr></table></figure>
<p>可以发现结果与<code>writeRowReadCol</code>相同。</p>
<h4 id="填充静态声明的共享内存">填充静态声明的共享内存</h4>
<p>使用前面提到的<a href="#内存填充">内存填充</a>来解决存储体冲突，并测试其性能。</p>
<figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs scss">                                                  		  耗时 	加载事务  存储事务<br><span class="hljs-built_in">writeRowReadColPadding</span>(int *) (<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<span class="hljs-built_in">x</span>(<span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">1</span>)		 <span class="hljs-number">896</span> ns		<span class="hljs-number">32</span>	  	<span class="hljs-number">32</span><br></code></pre></td></tr></table></figure>
<p>可以发现，通过填充完美的解决了存储体冲突。</p>
<h4 id="填充动态声明的共享内存">填充动态声明的共享内存</h4>
<p>实现基于动态共享内存填充的核函数，并测试其性能。</p>
<figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs scss">                                                  		  耗时 	加载事务  存储事务<br><span class="hljs-built_in">writeRowReadColDynPad</span>(int *) (<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<span class="hljs-built_in">x</span>(<span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">1</span>)		 <span class="hljs-number">896</span> ns		<span class="hljs-number">32</span>	  	<span class="hljs-number">32</span><br></code></pre></td></tr></table></figure>
<p>可以发现基于动态共享内存的填充也是有效的。</p>
<h3 id="矩形共享内存">矩形共享内存</h3>
<p>将上述的方形共享内存推广到矩形这个更为一般的情况。实现与上述功能相同的几个核函数，并分析其性能。</p>
<figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs scss">                                                  		  耗时	 加载事务  存储事务<br><span class="hljs-built_in">writeRowReadRow</span>(int *) (<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<span class="hljs-built_in">x</span>(<span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">1</span>)			<span class="hljs-number">1119</span> ns	  	  <span class="hljs-number">16</span>	  <span class="hljs-number">16</span><br><span class="hljs-built_in">writeColReadCol</span>(int *) (<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<span class="hljs-built_in">x</span>(<span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">1</span>)			<span class="hljs-number">1248</span> ns		 <span class="hljs-number">256</span>	 <span class="hljs-number">256</span><br><span class="hljs-built_in">writeColReadRow</span>(int *) (<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<span class="hljs-built_in">x</span>(<span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">1</span>)			 <span class="hljs-number">928</span> ns		  <span class="hljs-number">16</span>	 <span class="hljs-number">256</span><br><span class="hljs-built_in">writeRowReadCol</span>(int *) (<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<span class="hljs-built_in">x</span>(<span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">1</span>)			 <span class="hljs-number">992</span> ns		 <span class="hljs-number">256</span>	  <span class="hljs-number">16</span><br><span class="hljs-built_in">writeRowReadColDynamic</span>(int *) (<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<span class="hljs-built_in">x</span>(<span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">1</span>)		 <span class="hljs-number">960</span> ns		 <span class="hljs-number">256</span>	  <span class="hljs-number">16</span><br><span class="hljs-built_in">writeRowReadColPadding</span>(int *) (<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<span class="hljs-built_in">x</span>(<span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">1</span>)		 <span class="hljs-number">896</span> ns		  <span class="hljs-number">16</span>	  <span class="hljs-number">16</span><br><span class="hljs-built_in">writeRowReadColDynPad</span>(int *) (<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<span class="hljs-built_in">x</span>(<span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">1</span>)		 <span class="hljs-number">896</span> ns		  <span class="hljs-number">16</span>	  <span class="hljs-number">16</span><br></code></pre></td></tr></table></figure>
<blockquote>
<p>详细代码参考<a target="_blank" rel="noopener" href="https://github.com/Deleter-D/CUDA/blob/master/04_shared_and_constant_memory/02_rectangle_shared_memory.cu">rectangle_shared_memory.cu</a>。</p>
</blockquote>
<h2 id="减少全局内存访问">减少全局内存访问</h2>
<h3 id="使用共享内存的并行归约">使用共享内存的并行归约</h3>
<p>将之前实现的线程束展开的并行归约作为性能基准，利用共享内存的操作代替全军内存的原地操作，观察两者性能差距。</p>
<blockquote>
<p>详细代码参考<a target="_blank" rel="noopener" href="https://github.com/Deleter-D/CUDA/blob/master/04_shared_and_constant_memory/03_reduce_with_shared_memory.cu">reduce_with_shared_memory.cu</a>。</p>
</blockquote>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">Array</span> Size: <span class="hljs-number">16777216</span><br><span class="hljs-attribute">cpu</span> reduce      elapsed <span class="hljs-number">16</span>.<span class="hljs-number">0859</span> ms      cpu_sum: <span class="hljs-number">206464799</span><br><span class="hljs-attribute">gpu</span> Gemm        elapsed <span class="hljs-number">0</span>.<span class="hljs-number">384192</span> ms     gpu_sum: <span class="hljs-number">206464799</span>      &lt;&lt;&lt;<span class="hljs-number">131072</span>, <span class="hljs-number">128</span>&gt;&gt;&gt;<br><span class="hljs-attribute">gpu</span> Semm        elapsed <span class="hljs-number">0</span>.<span class="hljs-number">278624</span> ms     gpu_sum: <span class="hljs-number">206464799</span>      &lt;&lt;&lt;<span class="hljs-number">131072</span>, <span class="hljs-number">128</span>&gt;&gt;&gt;<br><span class="hljs-attribute">Result</span> correct!<br></code></pre></td></tr></table></figure>
<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">																		全局加载事务	全局存储事务<br>reduce<span class="hljs-constructor">Gmem(<span class="hljs-params">int</span> <span class="hljs-operator">*</span>, <span class="hljs-params">int</span> <span class="hljs-operator">*</span>, <span class="hljs-params">unsigned</span> <span class="hljs-params">int</span>)</span> (<span class="hljs-number">131072</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)x(<span class="hljs-number">128</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)		<span class="hljs-number">8912896</span>		<span class="hljs-number">4325376</span><br>reduce<span class="hljs-constructor">Smem(<span class="hljs-params">int</span> <span class="hljs-operator">*</span>, <span class="hljs-params">int</span> <span class="hljs-operator">*</span>, <span class="hljs-params">unsigned</span> <span class="hljs-params">int</span>)</span> (<span class="hljs-number">131072</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)x(<span class="hljs-number">128</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)		<span class="hljs-number">2097152</span>		 <span class="hljs-number">131072</span><br></code></pre></td></tr></table></figure>
<p>可以看到，通过使用共享内存代替全局内存的原地操作，大幅度减少了全局内存事务，从而提升了总体性能。</p>
<h3 id="使用展开的并行归约">使用展开的并行归约</h3>
<p>在使用共享内存进行归约的基础上，再利用技术展开，每个线程处理4个数据块的元素，再次对比它们的性能。</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">Array</span> Size: <span class="hljs-number">16777216</span><br><span class="hljs-attribute">cpu</span> reduce      elapsed <span class="hljs-number">16</span>.<span class="hljs-number">0439</span> ms      cpu_sum: <span class="hljs-number">206464799</span><br><span class="hljs-attribute">gpu</span> Gemm        elapsed <span class="hljs-number">0</span>.<span class="hljs-number">384</span> ms        gpu_sum: <span class="hljs-number">206464799</span>      &lt;&lt;&lt;<span class="hljs-number">131072</span>, <span class="hljs-number">128</span>&gt;&gt;&gt;<br><span class="hljs-attribute">gpu</span> Semm        elapsed <span class="hljs-number">0</span>.<span class="hljs-number">278464</span> ms     gpu_sum: <span class="hljs-number">206464799</span>      &lt;&lt;&lt;<span class="hljs-number">131072</span>, <span class="hljs-number">128</span>&gt;&gt;&gt;<br><span class="hljs-attribute">gpu</span> SemmUnroll  elapsed <span class="hljs-number">0</span>.<span class="hljs-number">214976</span> ms     gpu_sum: <span class="hljs-number">206464799</span>      &lt;&lt;&lt;<span class="hljs-number">32768</span>, <span class="hljs-number">128</span>&gt;&gt;&gt;<br><span class="hljs-attribute">Result</span> correct!<br></code></pre></td></tr></table></figure>
<p>分析其全局加载和存储事务。</p>
<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">																		全局加载事务	全局存储事务<br>reduce<span class="hljs-constructor">Gmem(<span class="hljs-params">int</span> <span class="hljs-operator">*</span>, <span class="hljs-params">int</span> <span class="hljs-operator">*</span>, <span class="hljs-params">unsigned</span> <span class="hljs-params">int</span>)</span> (<span class="hljs-number">131072</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)x(<span class="hljs-number">128</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)		<span class="hljs-number">8912896</span>		<span class="hljs-number">4325376</span><br>reduce<span class="hljs-constructor">Smem(<span class="hljs-params">int</span> <span class="hljs-operator">*</span>, <span class="hljs-params">int</span> <span class="hljs-operator">*</span>, <span class="hljs-params">unsigned</span> <span class="hljs-params">int</span>)</span> (<span class="hljs-number">131072</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)x(<span class="hljs-number">128</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)		<span class="hljs-number">2097152</span>		 <span class="hljs-number">131072</span><br>reduce<span class="hljs-constructor">SmemUnroll(<span class="hljs-params">int</span> <span class="hljs-operator">*</span>, <span class="hljs-params">int</span> <span class="hljs-operator">*</span>, <span class="hljs-params">unsigned</span> <span class="hljs-params">int</span>)</span> (<span class="hljs-number">32768</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)x(<span class="hljs-number">128</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)	<span class="hljs-number">2097152</span>		  <span class="hljs-number">32768</span><br></code></pre></td></tr></table></figure>
<p>观察发现，虽然全局加载事务并没有减少，但全局存储事务却减少到了原来的1/4。</p>
<h3 id="使用动态共享内存的并行归约">使用动态共享内存的并行归约</h3>
<p>上述基于共享内存的展开并行归约也可以使用动态共享内存，性能表现与使用静态共享内存时接近，这里不再赘述。</p>
<h2 id="合并的全局内存访问">合并的全局内存访问</h2>
<p>使用共享内存可以避免对未合并的全局内存进行访问，矩阵转置是一个典型的例子，读操作是合并的，但写操作是交叉访问的。</p>
<h3 id="朴素转置">朴素转置</h3>
<p>实现一个朴素转置，核函数如下。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-meta">#<span class="hljs-keyword">define</span> INDEX(ROW, COL, INNER) ((ROW) * (INNER) + (COL))</span><br><br><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">naiveGmem</span><span class="hljs-params">(<span class="hljs-type">float</span> *out, <span class="hljs-type">float</span> *in, <span class="hljs-type">const</span> <span class="hljs-type">int</span> rows, <span class="hljs-type">const</span> <span class="hljs-type">int</span> cols)</span></span><br><span class="hljs-function"></span>{<br>    <span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> col = blockIdx.x * blockDim.x + threadIdx.x;<br>    <span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> row = blockIdx.y * blockDim.y + threadIdx.y;<br><br>    <span class="hljs-keyword">if</span> (row &lt; rows &amp;&amp; col &lt; cols)<br>        out[<span class="hljs-built_in">INDEX</span>(col, row, rows)] = in[<span class="hljs-built_in">INDEX</span>(row, col, cols)];<br>}<br></code></pre></td></tr></table></figure>
<p>再实现一个读写操作都是合并访问的核函数，用来模拟性能的近似上界。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">copyGmem</span><span class="hljs-params">(<span class="hljs-type">float</span> *out, <span class="hljs-type">float</span> *in, <span class="hljs-type">const</span> <span class="hljs-type">int</span> rows, <span class="hljs-type">const</span> <span class="hljs-type">int</span> cols)</span></span><br><span class="hljs-function"></span>{<br>    <span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> col = blockIdx.x * blockDim.x + threadIdx.x;<br>    <span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> row = blockIdx.y * blockDim.y + threadIdx.y;<br><br>    <span class="hljs-keyword">if</span> (row &lt; rows &amp;&amp; col &lt; cols)<br>        out[<span class="hljs-built_in">INDEX</span>(row, col, cols)] = in[<span class="hljs-built_in">INDEX</span>(row, col, cols)];<br>}<br></code></pre></td></tr></table></figure>
<blockquote>
<p>详细代码参考<a target="_blank" rel="noopener" href="https://github.com/Deleter-D/CUDA/blob/master/04_shared_and_constant_memory/04_transpose_with_shared_memory.cu">transpose_with_shared_memory.cu</a>。</p>
</blockquote>
<p>这里使用<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="7.291ex" height="1.557ex" role="img" focusable="false" viewBox="0 -666 3222.4 688"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mn"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" transform="translate(500,0)"></path></g><g data-mml-node="mo" transform="translate(1222.2,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"></path></g><g data-mml-node="mn" transform="translate(2222.4,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path><path data-c="36" d="M42 313Q42 476 123 571T303 666Q372 666 402 630T432 550Q432 525 418 510T379 495Q356 495 341 509T326 548Q326 592 373 601Q351 623 311 626Q240 626 194 566Q147 500 147 364L148 360Q153 366 156 373Q197 433 263 433H267Q313 433 348 414Q372 400 396 374T435 317Q456 268 456 210V192Q456 169 451 149Q440 90 387 34T253 -22Q225 -22 199 -14T143 16T92 75T56 172T42 313ZM257 397Q227 397 205 380T171 335T154 278T148 216Q148 133 160 97T198 39Q222 21 251 21Q302 21 329 59Q342 77 347 104T352 209Q352 289 347 316T329 361Q302 397 257 397Z" transform="translate(500,0)"></path></g></g></g></svg></mjx-container></span>的二维线程块来调用，经过测试，上面两个核函数的性能表现如下。</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">Kernel</span>          Elapsed time    Effective bandwidth<br><span class="hljs-attribute">copyGmem</span>        <span class="hljs-number">0</span>.<span class="hljs-number">291488</span> ms     <span class="hljs-number">0</span>.<span class="hljs-number">460457</span> GB/s<br><span class="hljs-attribute">naiveGmem</span>       <span class="hljs-number">0</span>.<span class="hljs-number">994176</span> ms     <span class="hljs-number">0</span>.<span class="hljs-number">135004</span> GB/s<br></code></pre></td></tr></table></figure>
<p>分析其每次请求中的全局内存事务数量，结果如下。</p>
<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">																全局加载事务	全局存储事务<br>copy<span class="hljs-constructor">Gmem(<span class="hljs-params">float</span> <span class="hljs-operator">*</span>, <span class="hljs-params">float</span> <span class="hljs-operator">*</span>, <span class="hljs-params">int</span>, <span class="hljs-params">int</span>)</span> (<span class="hljs-number">256</span>, <span class="hljs-number">256</span>, <span class="hljs-number">1</span>)x(<span class="hljs-number">16</span>, <span class="hljs-number">16</span>, <span class="hljs-number">1</span>)		<span class="hljs-number">4</span>			 <span class="hljs-number">4</span><br>naive<span class="hljs-constructor">Gmem(<span class="hljs-params">float</span> <span class="hljs-operator">*</span>, <span class="hljs-params">float</span> <span class="hljs-operator">*</span>, <span class="hljs-params">int</span>, <span class="hljs-params">int</span>)</span> (<span class="hljs-number">256</span>, <span class="hljs-number">256</span>, <span class="hljs-number">1</span>)x(<span class="hljs-number">16</span>, <span class="hljs-number">16</span>, <span class="hljs-number">1</span>)		<span class="hljs-number">4</span>			<span class="hljs-number">32</span><br></code></pre></td></tr></table></figure>
<p>可以发现，由于朴素转置的写操作是交叉访问的，所以每次请求中的全局存储事务要更多。</p>
<h3 id="使用共享内存的矩阵转置">使用共享内存的矩阵转置</h3>
<p>可以使用二维共享内存来缓存原始矩阵的数据，从而避免交叉的全局内存写操作。首先从全局内存中读取块内的一行写入共享内存的一行，然后从共享内存读取一列写入全局内存的一行。</p>
<p><img src="https://github.com/Deleter-D/Images/assets/56388518/e10502de-f723-49c2-928c-d672958a7ec0"></p>
<p>实现该核函数时，需要注意索引的计算方式。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">transposeSmem</span><span class="hljs-params">(<span class="hljs-type">float</span> *out, <span class="hljs-type">float</span> *in, <span class="hljs-type">const</span> <span class="hljs-type">int</span> rows, <span class="hljs-type">const</span> <span class="hljs-type">int</span> cols)</span></span><br><span class="hljs-function"></span>{<br>    __shared__ <span class="hljs-type">float</span> tile[BDIMY][BDIMX];<br><br>    <span class="hljs-comment">// 原始矩阵索引</span><br>    <span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> col = blockIdx.x * blockDim.x + threadIdx.x;<br>    <span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> row = blockIdx.y * blockDim.y + threadIdx.y;<br><br>    <span class="hljs-keyword">if</span> (row &lt; rows &amp;&amp; col &lt; cols)<br>        tile[threadIdx.y][threadIdx.x] = in[<span class="hljs-built_in">INDEX</span>(row, col, cols)];<br><br>    <span class="hljs-comment">// 由于转置过程中，不仅block需要转置，block内的thread也需要转置</span><br>    <span class="hljs-comment">// 所以利用irow和icol来代替原来的threadIdx的x和y维度</span><br>    <span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> bidx = threadIdx.y * blockDim.x + threadIdx.x;<br>    <span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> irow = bidx / blockDim.y;<br>    <span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> icol = bidx % blockDim.y;<br><br>    <span class="hljs-comment">// 转置矩阵中，blockDim和blockIdx的x维度计算列索引，y维度计算行索引，与原始矩阵相反</span><br>    row = blockIdx.x * blockDim.x + irow;<br>    col = blockIdx.y * blockDim.y + icol;<br><br>    __syncthreads();<br><br>    <span class="hljs-keyword">if</span> (row &lt; cols &amp;&amp; col &lt; rows)<br>        out[<span class="hljs-built_in">INDEX</span>(row, col, rows)] = tile[icol][irow];<br>}<br></code></pre></td></tr></table></figure>
<p>分析其性能与全局内存事务。</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">Kernel</span>          Elapsed time    Effective bandwidth<br><span class="hljs-attribute">copyGmem</span>        <span class="hljs-number">0</span>.<span class="hljs-number">291072</span> ms     <span class="hljs-number">0</span>.<span class="hljs-number">461115</span> GB/s<br><span class="hljs-attribute">naiveGmem</span>       <span class="hljs-number">1</span>.<span class="hljs-number">007616</span> ms     <span class="hljs-number">0</span>.<span class="hljs-number">133203</span> GB/s<br><span class="hljs-attribute">transposeSmem</span>   <span class="hljs-number">0</span>.<span class="hljs-number">343520</span> ms     <span class="hljs-number">0</span>.<span class="hljs-number">390713</span> GB/s<br></code></pre></td></tr></table></figure>
<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">																	全局加载事务	全局存储事务<br>copy<span class="hljs-constructor">Gmem(<span class="hljs-params">float</span> <span class="hljs-operator">*</span>, <span class="hljs-params">float</span> <span class="hljs-operator">*</span>, <span class="hljs-params">int</span>, <span class="hljs-params">int</span>)</span> (<span class="hljs-number">256</span>, <span class="hljs-number">256</span>, <span class="hljs-number">1</span>)x(<span class="hljs-number">16</span>, <span class="hljs-number">16</span>, <span class="hljs-number">1</span>)			<span class="hljs-number">4</span>			 <span class="hljs-number">4</span><br>naive<span class="hljs-constructor">Gmem(<span class="hljs-params">float</span> <span class="hljs-operator">*</span>, <span class="hljs-params">float</span> <span class="hljs-operator">*</span>, <span class="hljs-params">int</span>, <span class="hljs-params">int</span>)</span> (<span class="hljs-number">256</span>, <span class="hljs-number">256</span>, <span class="hljs-number">1</span>)x(<span class="hljs-number">16</span>, <span class="hljs-number">16</span>, <span class="hljs-number">1</span>)			<span class="hljs-number">4</span>			<span class="hljs-number">32</span><br>transpose<span class="hljs-constructor">Smem(<span class="hljs-params">float</span> <span class="hljs-operator">*</span>, <span class="hljs-params">float</span> <span class="hljs-operator">*</span>, <span class="hljs-params">int</span>, <span class="hljs-params">int</span>)</span> (<span class="hljs-number">128</span>, <span class="hljs-number">256</span>, <span class="hljs-number">1</span>)x(<span class="hljs-number">32</span>, <span class="hljs-number">16</span>, <span class="hljs-number">1</span>)		<span class="hljs-number">4</span>			 <span class="hljs-number">4</span><br></code></pre></td></tr></table></figure>
<p>上面提到了，这种方式虽然在共享内存中读取列的时候依然会发生存储体冲突，但这样的结果已经比直接对全局内存进行交叉写入要好的多。共享内存中的存储体冲突可以通过分析其共享内存事务数量来解释。</p>
<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">																	   共享加载事务	共享存储事务<br>transpose<span class="hljs-constructor">Smem(<span class="hljs-params">float</span> <span class="hljs-operator">*</span>, <span class="hljs-params">float</span> <span class="hljs-operator">*</span>, <span class="hljs-params">int</span>, <span class="hljs-params">int</span>)</span> (<span class="hljs-number">128</span>, <span class="hljs-number">256</span>, <span class="hljs-number">1</span>)x(<span class="hljs-number">32</span>, <span class="hljs-number">16</span>, <span class="hljs-number">1</span>)		<span class="hljs-number">8460165</span>		  <span class="hljs-number">533345</span><br></code></pre></td></tr></table></figure>
<h3 id="使用填充共享内存的矩阵转置">使用填充共享内存的矩阵转置</h3>
<p>使用之前提到的<a href="#内存填充">内存填充</a>技术来优化上面的核函数。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">transposeSmemPad</span><span class="hljs-params">(<span class="hljs-type">float</span> *out, <span class="hljs-type">float</span> *in, <span class="hljs-type">int</span> rows, <span class="hljs-type">int</span> cols)</span></span><br><span class="hljs-function"></span>{<br>    __shared__ <span class="hljs-type">float</span> tile[BDIMY][BDIMX + PAD];<br><br>    <span class="hljs-comment">// 原始矩阵索引</span><br>    <span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> col = blockIdx.x * blockDim.x + threadIdx.x;<br>    <span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> row = blockIdx.y * blockDim.y + threadIdx.y;<br><br>    <span class="hljs-keyword">if</span> (row &lt; rows &amp;&amp; col &lt; cols)<br>        tile[threadIdx.y][threadIdx.x] = in[<span class="hljs-built_in">INDEX</span>(row, col, cols)];<br><br>    <span class="hljs-comment">// 转置block中的线程索引</span><br>    <span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> bidx = threadIdx.y * blockDim.x + threadIdx.x;<br>    <span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> irow = bidx / blockDim.y;<br>    <span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> icol = bidx % blockDim.y;<br><br>    row = blockIdx.x * blockDim.x + irow;<br>    col = blockIdx.y * blockDim.y + icol;<br><br>    __syncthreads();<br><br>    <span class="hljs-keyword">if</span> (row &lt; cols &amp;&amp; col &lt; rows)<br>        out[<span class="hljs-built_in">INDEX</span>(row, col, rows)] = tile[icol][irow];<br>}<br></code></pre></td></tr></table></figure>
<p>这里选择填充2个位置，这样可以完全消除共享内存的存储体冲突。可以通过分析其性能及共享内存事务来证明这一点。</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">Kernel</span>				Elapsed time	Effective bandwidth<br><span class="hljs-attribute">copyGmem</span>			<span class="hljs-number">0</span>.<span class="hljs-number">292448</span> ms		<span class="hljs-number">0</span>.<span class="hljs-number">458946</span> GB/s<br><span class="hljs-attribute">naiveGmem</span>			<span class="hljs-number">0</span>.<span class="hljs-number">987136</span> ms		<span class="hljs-number">0</span>.<span class="hljs-number">135967</span> GB/s<br><span class="hljs-attribute">transposeSmem</span>		<span class="hljs-number">0</span>.<span class="hljs-number">404192</span> ms		<span class="hljs-number">0</span>.<span class="hljs-number">332064</span> GB/s<br><span class="hljs-attribute">transposeSmemPad</span>	<span class="hljs-number">0</span>.<span class="hljs-number">326272</span> ms		<span class="hljs-number">0</span>.<span class="hljs-number">411368</span> GB/s<br></code></pre></td></tr></table></figure>
<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs reasonml">																	   共享加载事务	共享存储事务<br>transpose<span class="hljs-constructor">Smem(<span class="hljs-params">float</span> <span class="hljs-operator">*</span>, <span class="hljs-params">float</span> <span class="hljs-operator">*</span>, <span class="hljs-params">int</span>, <span class="hljs-params">int</span>)</span> (<span class="hljs-number">128</span>, <span class="hljs-number">256</span>, <span class="hljs-number">1</span>)x(<span class="hljs-number">32</span>, <span class="hljs-number">16</span>, <span class="hljs-number">1</span>)		<span class="hljs-number">8460165</span>		  <span class="hljs-number">533345</span><br>transpose<span class="hljs-constructor">SmemPad(<span class="hljs-params">float</span> <span class="hljs-operator">*</span>, <span class="hljs-params">float</span> <span class="hljs-operator">*</span>, <span class="hljs-params">int</span>, <span class="hljs-params">int</span>)</span> (<span class="hljs-number">128</span>, <span class="hljs-number">256</span>, <span class="hljs-number">1</span>)x(<span class="hljs-number">32</span>, <span class="hljs-number">16</span>, <span class="hljs-number">1</span>)	 <span class="hljs-number">545540</span>		  <span class="hljs-number">533686</span><br></code></pre></td></tr></table></figure>
<h3 id="使用展开的矩阵转置">使用展开的矩阵转置</h3>
<p>在上述使用了内存填充的核函数基础上，使用展开技术进行优化，使每个线程处理两个元素。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">transposeSmemUnrollPad</span><span class="hljs-params">(<span class="hljs-type">float</span> *out, <span class="hljs-type">float</span> *in, <span class="hljs-type">int</span> rows, <span class="hljs-type">int</span> cols)</span></span><br><span class="hljs-function"></span>{<br>    <span class="hljs-comment">// 使用一维的共享内存</span><br>    __shared__ <span class="hljs-type">float</span> tile[BDIMY][BDIMX * <span class="hljs-number">2</span> + PAD];<br><br>    <span class="hljs-comment">// 原始矩阵索引</span><br>    <span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> col = <span class="hljs-number">2</span> * blockIdx.x * blockDim.x + threadIdx.x;<br>    <span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> row = blockIdx.y * blockDim.y + threadIdx.y;<br><br>    <span class="hljs-keyword">if</span> (row &lt; rows &amp;&amp; col + blockDim.x &lt; cols)<br>    {<br>        tile[threadIdx.y][threadIdx.x] = in[<span class="hljs-built_in">INDEX</span>(row, col, cols)];<br>        tile[threadIdx.y][threadIdx.x + blockDim.x] = in[<span class="hljs-built_in">INDEX</span>(row, col + blockDim.x, cols)];<br>    }<br><br>    <span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> bidx = threadIdx.y * blockDim.x + threadIdx.x;<br>    <span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> irow = bidx / blockDim.y;<br>    <span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> icol = bidx % blockDim.y;<br><br>    row = <span class="hljs-number">2</span> * blockIdx.x * blockDim.x + irow;<br>    col = blockIdx.y * blockDim.y + icol;<br>    <br>    __syncthreads();<br><br>    <span class="hljs-keyword">if</span> (row + blockDim.x &lt; cols &amp;&amp; col &lt; rows)<br>    {<br>        out[<span class="hljs-built_in">INDEX</span>(row, col, rows)] = tile[icol][irow];<br>        out[<span class="hljs-built_in">INDEX</span>(row + blockDim.x, col, rows)] = tile[icol][irow + blockDim.x];<br>    }<br>}<br></code></pre></td></tr></table></figure>
<p>进行性能对比后发现，相比使用内存填充的核函数，有略微的提升。</p>
<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">Kernel</span>					Elapsed time	Effective bandwidth<br><span class="hljs-attribute">copyGmem</span>				<span class="hljs-number">0</span>.<span class="hljs-number">292864</span> ms		<span class="hljs-number">0</span>.<span class="hljs-number">458294</span> GB/s<br><span class="hljs-attribute">naiveGmem</span>				<span class="hljs-number">0</span>.<span class="hljs-number">987008</span> ms		<span class="hljs-number">0</span>.<span class="hljs-number">135984</span> GB/s<br><span class="hljs-attribute">transposeSmem</span>			<span class="hljs-number">0</span>.<span class="hljs-number">377856</span> ms		<span class="hljs-number">0</span>.<span class="hljs-number">355209</span> GB/s<br><span class="hljs-attribute">transposeSmemPad</span>		<span class="hljs-number">0</span>.<span class="hljs-number">326656</span> ms		<span class="hljs-number">0</span>.<span class="hljs-number">410884</span> GB/s<br><span class="hljs-attribute">transposeSmemUnrollPad</span>	<span class="hljs-number">0</span>.<span class="hljs-number">322560</span> ms		<span class="hljs-number">0</span>.<span class="hljs-number">416102</span> GB/s<br></code></pre></td></tr></table></figure>
<p>使用展开技术使得更多的内存请求将同时处于运行状态，且会提高读写吞吐量。<code>ncu</code>分析设备内存的读写吞吐量可以佐证这一点。</p>
<figure class="highlight scss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs scss">																	   		 读吞吐量(GB/s)	  写吞吐量(GB/s)<br><span class="hljs-built_in">naiveGmem</span>(float *, float *, int, int) (<span class="hljs-number">128</span>, <span class="hljs-number">256</span>, <span class="hljs-number">1</span>)<span class="hljs-built_in">x</span>(<span class="hljs-number">32</span>, <span class="hljs-number">16</span>, <span class="hljs-number">1</span>)					<span class="hljs-number">55.81</span>			<span class="hljs-number">52.64</span><br><span class="hljs-built_in">transposeSmem</span>(float *, float *, int, int) (<span class="hljs-number">128</span>, <span class="hljs-number">256</span>, <span class="hljs-number">1</span>)<span class="hljs-built_in">x</span>(<span class="hljs-number">32</span>, <span class="hljs-number">16</span>, <span class="hljs-number">1</span>)				<span class="hljs-number">193.00</span>		 	<span class="hljs-number">177.18</span><br><span class="hljs-built_in">transposeSmemPad</span>(float *, float *, int, int) (<span class="hljs-number">128</span>, <span class="hljs-number">256</span>, <span class="hljs-number">1</span>)<span class="hljs-built_in">x</span>(<span class="hljs-number">32</span>, <span class="hljs-number">16</span>, <span class="hljs-number">1</span>)	 		<span class="hljs-number">205.98</span>		  	<span class="hljs-number">178.52</span><br><span class="hljs-built_in">transposeSmemUnrollPad</span>(float *, float *, int, int) (<span class="hljs-number">64</span>, <span class="hljs-number">256</span>, <span class="hljs-number">1</span>)<span class="hljs-built_in">x</span>(<span class="hljs-number">32</span>, <span class="hljs-number">16</span>, <span class="hljs-number">1</span>)		<span class="hljs-number">206.65</span>			<span class="hljs-number">186.28</span><br></code></pre></td></tr></table></figure>
<h2 id="常量内存">常量内存</h2>
<p>常量内存对于核函数来说是只读的，但对于主机来说是可读可写的。常量内存位于设备的DRAM上（与全局内存一样），且有一个专用的片上缓存。与一级缓存和共享内存类似，从每个SM的常量缓存中读取的延迟，比直接从常量内存中读取的延迟低得多。每个SM常量缓存大小限制为64KB。</p>
<p>常量内存与之前提到的所有类型的内存有着不同的最优访问模式。在常量内存中，若线程束中的所有线程都访问相同的位置，则这个访问模式是最优的。若线程束中的线程访问不同地址，则访问需要串行。</p>
<p>在全局作用域中必须使用<code>__constant__</code>修饰符来声明常量变量。常量内存变量的生命周期与应用程序的生命周期相同，对所有线程都是可访问的，并且可以通过运行时函数对主机也可访问。</p>
<p>由于设备只能读取常量内存，所以常量内存中的值必须通过运行时函数<code>cudaMemcpyToSymbol()</code>来初始化。</p>
<h3 id="使用常量内存实现一维模板">使用常量内存实现一维模板</h3>
<p>在数值分析中，模板计算在点的集合上应用一个函数，并使用该函数的输出更新单一点的值。在一维中，位置<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="1.294ex" height="1.025ex" role="img" focusable="false" viewBox="0 -442 572 453"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g></g></g></svg></mjx-container></span>周围的的九点模板会给如下位置上的值应用一些函数。 <span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="61.296ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 27092.9 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="7B" d="M434 -231Q434 -244 428 -250H410Q281 -250 230 -184Q225 -177 222 -172T217 -161T213 -148T211 -133T210 -111T209 -84T209 -47T209 0Q209 21 209 53Q208 142 204 153Q203 154 203 155Q189 191 153 211T82 231Q71 231 68 234T65 250T68 266T82 269Q116 269 152 289T203 345Q208 356 208 377T209 529V579Q209 634 215 656T244 698Q270 724 324 740Q361 748 377 749Q379 749 390 749T408 750H428Q434 744 434 732Q434 719 431 716Q429 713 415 713Q362 710 332 689T296 647Q291 634 291 499V417Q291 370 288 353T271 314Q240 271 184 255L170 250L184 245Q202 239 220 230T262 196T290 137Q291 131 291 1Q291 -134 296 -147Q306 -174 339 -192T415 -213Q429 -213 431 -216Q434 -219 434 -231Z"></path></g><g data-mml-node="mi" transform="translate(500,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(1294.2,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(2294.4,0)"><path data-c="34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z"></path></g><g data-mml-node="mi" transform="translate(2794.4,0)"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="mo" transform="translate(3370.4,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(3815.1,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(4609.3,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(5609.6,0)"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path></g><g data-mml-node="mi" transform="translate(6109.6,0)"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="mo" transform="translate(6685.6,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(7130.2,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(7924.4,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(8924.7,0)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="mi" transform="translate(9424.7,0)"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="mo" transform="translate(10000.7,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(10445.3,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(11239.6,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mi" transform="translate(12239.8,0)"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="mo" transform="translate(12815.8,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(13260.4,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(13832.4,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(14277.1,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(15071.3,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(16071.6,0)"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="mo" transform="translate(16647.6,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(17092.2,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(17886.4,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mn" transform="translate(18886.7,0)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="mi" transform="translate(19386.7,0)"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="mo" transform="translate(19962.7,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(20407.3,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(21201.6,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mn" transform="translate(22201.8,0)"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path></g><g data-mml-node="mi" transform="translate(22701.8,0)"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="mo" transform="translate(23277.8,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(23722.4,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(24516.7,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mn" transform="translate(25516.9,0)"><path data-c="34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z"></path></g><g data-mml-node="mi" transform="translate(26016.9,0)"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="mo" transform="translate(26592.9,0)"><path data-c="7D" d="M65 731Q65 745 68 747T88 750Q171 750 216 725T279 670Q288 649 289 635T291 501Q292 362 293 357Q306 312 345 291T417 269Q428 269 431 266T434 250T431 234T417 231Q380 231 345 210T298 157Q293 143 292 121T291 -28V-79Q291 -134 285 -156T256 -198Q202 -250 89 -250Q71 -250 68 -247T65 -230Q65 -224 65 -223T66 -218T69 -214T77 -213Q91 -213 108 -210T146 -200T183 -177T207 -139Q208 -134 209 3L210 139Q223 196 280 230Q315 247 330 250Q305 257 280 270Q225 304 212 352L210 362L209 498Q208 635 207 640Q195 680 154 696T77 713Q68 713 67 716T65 731Z"></path></g></g></g></svg></mjx-container></span> 我们不需要理解这个公式的实际意义，只需要观察到它会将上述的九个点作为输入，产生单一输出。下面使用一个实际公式作为示例。 <span class="math display"><mjx-container class="MathJax" jax="SVG" display="true"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="116.061ex" height="2.396ex" role="img" focusable="false" viewBox="0 -809 51298.9 1059"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mo" transform="translate(636,413) scale(0.707)"><path data-c="2032" d="M79 43Q73 43 52 49T30 61Q30 68 85 293T146 528Q161 560 198 560Q218 560 240 545T262 501Q262 496 260 486Q259 479 173 263T84 45T79 43Z"></path></g></g><g data-mml-node="mo" transform="translate(880.5,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1269.5,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(1841.5,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(2508.2,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="msub" transform="translate(3564,0)"><g data-mml-node="mi"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g><g data-mml-node="mn" transform="translate(466,-150) scale(0.707)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g></g><g data-mml-node="mo" transform="translate(4433.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(4822.6,0)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mo" transform="translate(5372.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(5761.6,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(6555.8,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mn" transform="translate(7556,0)"><path data-c="34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z"></path></g><g data-mml-node="mi" transform="translate(8056,0)"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="mo" transform="translate(8632,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(9243.2,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mi" transform="translate(10243.5,0)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mo" transform="translate(10793.5,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(11182.5,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(11976.7,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(12976.9,0)"><path data-c="34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z"></path></g><g data-mml-node="mi" transform="translate(13476.9,0)"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="mo" transform="translate(14052.9,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(14441.9,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(15053.1,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="msub" transform="translate(16053.3,0)"><g data-mml-node="mi"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g><g data-mml-node="mn" transform="translate(466,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(16922.9,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(17311.9,0)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mo" transform="translate(17861.9,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(18250.9,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(19045.1,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mn" transform="translate(20045.3,0)"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path></g><g data-mml-node="mi" transform="translate(20545.3,0)"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="mo" transform="translate(21121.3,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(21732.6,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mi" transform="translate(22732.8,0)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mo" transform="translate(23282.8,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(23671.8,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(24466,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(25466.2,0)"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path></g><g data-mml-node="mi" transform="translate(25966.2,0)"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="mo" transform="translate(26542.2,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(26931.2,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(27542.5,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="msub" transform="translate(28542.7,0)"><g data-mml-node="mi"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g><g data-mml-node="mn" transform="translate(466,-150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mo" transform="translate(29412.2,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(29801.2,0)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mo" transform="translate(30351.2,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(30740.2,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(31534.4,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mn" transform="translate(32534.7,0)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="mi" transform="translate(33034.7,0)"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="mo" transform="translate(33610.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(34221.9,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mi" transform="translate(35222.1,0)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mo" transform="translate(35772.1,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(36161.1,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(36955.3,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mn" transform="translate(37955.6,0)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g><g data-mml-node="mi" transform="translate(38455.6,0)"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="mo" transform="translate(39031.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(39420.6,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(40031.8,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="msub" transform="translate(41032,0)"><g data-mml-node="mi"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g><g data-mml-node="mn" transform="translate(466,-150) scale(0.707)"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path></g></g><g data-mml-node="mo" transform="translate(41901.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(42290.6,0)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mo" transform="translate(42840.6,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(43229.6,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(44023.8,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path></g><g data-mml-node="mi" transform="translate(45024,0)"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="mo" transform="translate(45600,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(46211.2,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mi" transform="translate(47211.4,0)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"></path></g><g data-mml-node="mo" transform="translate(47761.4,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(48150.4,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"></path></g><g data-mml-node="mo" transform="translate(48944.7,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path></g><g data-mml-node="mi" transform="translate(49944.9,0)"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g><g data-mml-node="mo" transform="translate(50520.9,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g><g data-mml-node="mo" transform="translate(50909.9,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container></span> 可以比较容易的观察到，公式中<span class="math inline"><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="10.887ex" height="1.439ex" role="img" focusable="false" viewBox="0 -442 4812.2 636"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g><g data-mml-node="mn" transform="translate(466,-150) scale(0.707)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g></g><g data-mml-node="mo" transform="translate(869.6,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(1314.2,0)"><g data-mml-node="mi"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g><g data-mml-node="mn" transform="translate(466,-150) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g><g data-mml-node="mo" transform="translate(2183.8,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(2628.4,0)"><g data-mml-node="mi"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g><g data-mml-node="mn" transform="translate(466,-150) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"></path></g></g><g data-mml-node="mo" transform="translate(3498,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="msub" transform="translate(3942.7,0)"><g data-mml-node="mi"><path data-c="1D450" d="M34 159Q34 268 120 355T306 442Q362 442 394 418T427 355Q427 326 408 306T360 285Q341 285 330 295T319 325T330 359T352 380T366 386H367Q367 388 361 392T340 400T306 404Q276 404 249 390Q228 381 206 359Q162 315 142 235T121 119Q121 73 147 50Q169 26 205 26H209Q321 26 394 111Q403 121 406 121Q410 121 419 112T429 98T420 83T391 55T346 25T282 0T202 -11Q127 -11 81 37T34 159Z"></path></g><g data-mml-node="mn" transform="translate(466,-150) scale(0.707)"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"></path></g></g></g></g></svg></mjx-container></span>这些系数是不变的，所以很适合存入常量内存中。且线程束中的所有线程都是访问这几个常量，这恰好满足常量内存的最优访问模式。</p>
<p>计算过程如下图所示。</p>
<p><img src="https://github.com/Deleter-D/Images/assets/56388518/64aa9167-0992-4040-bd66-786758dbcd9b"></p>
<p>借助共享内存来缓存数据，同时在其两侧添加一些光环数据，类似于卷积中的填充操作，是为了计算的合法性。通过如下核函数来实现整个计算过程。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">stancli1DGPU</span><span class="hljs-params">(<span class="hljs-type">float</span>* in, <span class="hljs-type">float</span>* out, <span class="hljs-type">int</span> size)</span></span><br><span class="hljs-function"></span>{<br>    <span class="hljs-comment">// 包含光环的共享内存</span><br>    __shared__ <span class="hljs-type">float</span> smem[BDIM + <span class="hljs-number">2</span> * RADIUS];<br><br>    <span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> idx = blockIdx.x * blockDim.x + threadIdx.x + RADIUS;<br><br>    <span class="hljs-keyword">while</span> (idx &lt; size + RADIUS)<br>    {<br>        <span class="hljs-comment">// 共享内存的索引，为模板计算作准备</span><br>        <span class="hljs-type">int</span> sidx = threadIdx.x + RADIUS;<br><br>        <span class="hljs-comment">// 将数据部分写入共享内存</span><br>        smem[sidx] = in[idx];<br><br>        <span class="hljs-comment">// 将光环部分度写入共享内存</span><br>        <span class="hljs-keyword">if</span> (threadIdx.x &lt; RADIUS)<br>        {<br>            smem[sidx - RADIUS] = in[idx - RADIUS];<br>            smem[sidx + BDIM]   = in[idx + BDIM];<br>        }<br><br>        __syncthreads();<br><br>        <span class="hljs-type">float</span> tmp = <span class="hljs-number">0.0f</span>;<br><br><span class="hljs-meta">#<span class="hljs-keyword">pragma</span> unroll</span><br>        <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i = <span class="hljs-number">1</span>; i &lt;= RADIUS; i++)<br>        {<br>            tmp += coef[i] * (smem[sidx + i] - smem[sidx - i]);<br>        }<br><br>        out[idx] = tmp;<br><br>        idx += gridDim.x * blockDim.x;<br>    }<br>}<br></code></pre></td></tr></table></figure>
<blockquote>
<p>详细代码参考<a target="_blank" rel="noopener" href="https://github.com/Deleter-D/CUDA/blob/master/04_shared_and_constant_memory/05_constant_stencli.cu">constant_stencli.cu</a>。</p>
</blockquote>
<h3 id="与只读缓存比较">与只读缓存比较</h3>
<p>只读缓存实质上是GPU的纹理流水线，用于存储全局内存中的数据。只读缓存是独立的，它拥有从标准全局内存读取的独立内存带宽，所以使用只读缓存可以为受制于内存带宽的核函数提供一些性能优势。</p>
<p>只读缓存不同于常量内存，其最优访问模式是线程束中的线程访问不同的位置。只读缓存的粒度为32字节。</p>
<p>当通过只读缓存访问全局内存时，需要在核函数中向编译器指出数据是只读的，可以通过如下两种方式：</p>
<ul>
<li>内部函数<code>__ldg()</code>；</li>
<li>全局内存的限定指针；</li>
</ul>
<p>内部函数<code>__ldg()</code>用于代替标准指针解引用，并强制加载通过只读数据缓存。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">kernel</span><span class="hljs-params">(<span class="hljs-type">float</span>* output, <span class="hljs-type">float</span>* input)</span> </span>{<br>    ...<br>    output[idx] += __ldg(&amp;input[idx]);<br>    ...<br>}<br></code></pre></td></tr></table></figure>
<p>也可以限定指针为<code>const __restrict__</code>，以表明它应该通过只读缓存被访问。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">kernel</span><span class="hljs-params">(<span class="hljs-type">float</span>* output, <span class="hljs-type">const</span> <span class="hljs-type">float</span>* __restrict__ input)</span> </span>{<br>	...<br>    output[idx] += input[idx];<br>    ...<br>}<br></code></pre></td></tr></table></figure>
<p>在只读缓存需要很多显式控制，或代码非常复杂以至于编译器无法检测到只读缓存的使用是否安全的情况下，内部函数<code>__ldg()</code>是更好的选择。</p>
<p>通过只读缓存加载的数据可以比较大，且能够在一个非统一的模式下进行访问。利用只读缓存来实现上述模板算法的核函数，唯一的区别就是函数声明。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-function">__global__ <span class="hljs-type">void</span> <span class="hljs-title">stancliReadOnly</span><span class="hljs-params">(<span class="hljs-type">float</span>* in, <span class="hljs-type">float</span>* out, <span class="hljs-type">int</span> size, <span class="hljs-type">const</span> <span class="hljs-type">float</span>* __restrict__ dcoef)</span></span><br><span class="hljs-function"></span>{<br>    ...<br>}<br></code></pre></td></tr></table></figure>
<p>但要注意的是，不同于常量内存，在核函数调用之前，必须提前分配只读缓存的设备内存。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-type">const</span> <span class="hljs-type">float</span> h_coef[] = {a0, a1, a2, a3, a4};<br><br><span class="hljs-comment">// 使用常量内存只需要调用如下运行时API，无需申请设备内存</span><br><span class="hljs-built_in">ERROR_CHECK</span>(<span class="hljs-built_in">cudaMemcpyToSymbol</span>(coef, h_coef, (RADIUS + <span class="hljs-number">1</span>) * <span class="hljs-built_in">sizeof</span>(<span class="hljs-type">float</span>)));<br><br><span class="hljs-comment">// 使用只读缓存时需要提前申请设备内存</span><br><span class="hljs-type">float</span>* d_coef;<br><span class="hljs-built_in">ERROR_CHECK</span>(<span class="hljs-built_in">cudaMalloc</span>((<span class="hljs-type">void</span>**)&amp;d_coef, (RADIUS + <span class="hljs-number">1</span>) * <span class="hljs-built_in">sizeof</span>(<span class="hljs-type">float</span>)));<br><span class="hljs-built_in">ERROR_CHECK</span>(<span class="hljs-built_in">cudaMemcpy</span>(d_coef, h_coef, (RADIUS + <span class="hljs-number">1</span>) * <span class="hljs-built_in">sizeof</span>(<span class="hljs-type">float</span>), cudaMemcpyHostToDevice));<br></code></pre></td></tr></table></figure>
<p>使用<code>nsys</code>统计两个核函数的耗时可以观察到，对于以广播模式访问的数据来说，常量内存是更适合的。</p>
<figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs pgsql"><span class="hljs-type">Time</span> (%)  Total <span class="hljs-type">Time</span> (ns)                         <span class="hljs-type">Name</span>                         <br><span class="hljs-comment">--------  ---------------  -----------------------------------------------------</span><br>    <span class="hljs-number">50.2</span>          <span class="hljs-number">408</span>,<span class="hljs-number">376</span>  stancliReadOnly(<span class="hljs-type">float</span> *, <span class="hljs-type">float</span> *, <span class="hljs-type">int</span>, const <span class="hljs-type">float</span> *)<br>    <span class="hljs-number">49.8</span>          <span class="hljs-number">405</span>,<span class="hljs-number">463</span>  stancliConstant(<span class="hljs-type">float</span> *, <span class="hljs-type">float</span> *, <span class="hljs-type">int</span>)<br></code></pre></td></tr></table></figure>
<h2 id="线程束洗牌指令">线程束洗牌指令</h2>
<p>从计算能力3.0开始加入了一种机制称为洗牌指令（shuffle instruction），只要两个线程在相同的线程束中，就允许这两个线程直接读取另一个线程的寄存器。这种直接的数据交换不是通过共享内存或全局内存来进行的，拥有比共享内存更低的延迟，且在执行数据交换时不消耗额外的内存。</p>
<p>这里引入一个概念——束内线程（lane），线程束中的每一个线程都是束内线程，每个束内线程都有一个唯一的束内线程索引。在一维线程块中，对于一个给定线程的束内线程索引和线程束索引可以通过如下方式计算。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs cpp">laneID = threadIdx.x % <span class="hljs-number">32</span>;<br>warpID = threadIdx.x / <span class="hljs-number">32</span>;<br></code></pre></td></tr></table></figure>
<p>对于多维的线程块，可以将多维线程坐标转换为一维线程索引，再应用上述公式来计算。</p>
<h3 id="线程束洗牌指令的不同形式">线程束洗牌指令的不同形式</h3>
<p>洗牌指令共有两组，一组用于整型变量，另一组用于浮点型变量。每组有4种形式的洗牌指令：</p>
<ul>
<li>广播传递；</li>
<li>向上传递；</li>
<li>向下传递；</li>
<li>异或传递。</li>
</ul>
<h4 id="广播传递">广播传递</h4>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-type">int</span> __shfl(<span class="hljs-type">int</span> var, <span class="hljs-type">int</span> srcLane, <span class="hljs-type">int</span> width=warpSize);<br></code></pre></td></tr></table></figure>
<p>其中<code>var</code>是待传递的变量，<code>srcLane</code>是提高该变量的线程的束内线程索引。最后一个参数<code>width</code>允许将线程束进一步划分为段，每段包含<code>width</code>个线程，取值范围是<code>[2, 32]</code>，每个段上会执行独立的洗牌操作。对于非32的其他<code>width</code>值，线程的束内线程索引可以通过<code>threadIdx.x % width</code>来确定。</p>
<p><img src="https://github.com/Deleter-D/Images/assets/56388518/4de2e52e-0207-4644-86a2-392b55b5c9f6"></p>
<p>上图展示了<code>__shfl(val, 2)</code>的调用示例。</p>
<h4 id="向上传递和向下传递">向上传递和向下传递</h4>
<p>向上传递和向下传递非常类似，两者的区别仅是传递方向不同。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-type">int</span> __shfl_up(<span class="hljs-type">int</span> var, <span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> delta, <span class="hljs-type">int</span> width=warpSize);<br><span class="hljs-type">int</span> __shfl_down(<span class="hljs-type">int</span> var, <span class="hljs-type">unsigned</span> <span class="hljs-type">int</span> delta, <span class="hljs-type">int</span> width=warpSize);<br></code></pre></td></tr></table></figure>
<p><code>delta</code>参数用来计算提供变量的束内线程索引：</p>
<ul>
<li>向上传递时，当前束内线程接受来自于束内线程索引为<code>当前束内线程索引 - delta</code>线程中的变量<code>var</code>；</li>
<li>向下传递时，当前束内线程接受来自于束内线程索引为<code>当前束内线程索引 + delta</code>线程中的变量<code>var</code>；</li>
</ul>
<p>若向上或向下传递过程中没有对应的源束内线程，则线程中的变量保持不变。</p>
<p><img src="https://github.com/Deleter-D/Images/assets/56388518/24efc65e-2556-49a2-881c-abbe3baea285"></p>
<p><img src="https://github.com/Deleter-D/Images/assets/56388518/2dc7ebab-592a-423e-9a10-518c46fb3c4f"></p>
<h4 id="异或传递">异或传递</h4>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cpp"><span class="hljs-type">int</span> __shfl_xor(<span class="hljs-type">int</span> var, <span class="hljs-type">int</span> laneMask, <span class="hljs-type">int</span> width=warpSize);<br></code></pre></td></tr></table></figure>
<p>异或传递较为特殊，它根据自身的束内线程索引与<code>laneMask</code>按位异或来确定源束内线程。</p>
<p><img src="https://github.com/Deleter-D/Images/assets/56388518/1ba1eb0e-524c-4cc1-b96b-e49d5b763f9c"></p>
<p>上图展示了蝴蝶寻址模式的示例。</p>
<p>上面提到的4种洗牌指令均有单精度浮点数版本。</p>
<blockquote>
<p><strong>注意：在6.0以后的PTX ISA中，已经不再支持这一系列的洗牌指令，新版本改为了带有<code>sync</code>的洗牌指令。</strong></p>
<p>上述的几个函数整体变化不大，只是在参数列表的最前面添加了一个参数<code>mask</code>：</p>
<ul>
<li><code>int __shfl_sync(unsigned mask, int var, int srcLane, int width=warpSize);</code></li>
<li><code>int __shfl_up_sync(unsigned mask, int var, int srcLane, int width=warpSize);</code></li>
<li><code>int __shfl_down_sync(unsigned mask, int var, int srcLane, int width=warpSize);</code></li>
<li><code>int __shfl_xor_sync(unsigned mask, int var, int srcLane, int width=warpSize);</code></li>
</ul>
<p>当然也有对应的单精度浮点数版本，并且拥有64位的版本，即<code>long</code>与<code>double</code>类型的接口。</p>
<p>该<code>mask</code>参数用于指定参与洗牌指令的束内线程，每个<code>bit</code>代表一个束内线程。<code>mask</code>给予编译器一个提示，为了保证正确性，所指向的束内线程必须全部参与洗牌指令，这样编译器就会生成一些必要的指令将这些线程重新汇聚起来。</p>
<p>简单来说，若使用默认的线程束大小，想要使所有束内线程都参与洗牌指令，则<code>mask</code>指定为<code>0xffffffff</code>即可。</p>
<p>对于<code>mask</code>参数，英伟达官方论坛有一个<a target="_blank" rel="noopener" href="https://forums.developer.nvidia.com/t/what-does-mask-mean-in-warp-shuffle-functions-shfl-sync/67697">帖子</a>对此进行了描述。</p>
<p>以及<a target="_blank" rel="noopener" href="https://github.com/Deleter-D/CUDA/blob/master/04_shared_and_constant_memory/06_shuffle_instruction.cu">shuffle_instruction.cu</a>和<a target="_blank" rel="noopener" href="https://github.com/Deleter-D/CUDA/blob/master/04_shared_and_constant_memory/07_reduce_with_shuffle.cu">reduce_with_shuffle.cu</a>有一系列详细的示例。</p>
</blockquote>
</div><div class="article-licensing box"><div class="licensing-title"><p>CUDA编程——共享内存和常量内存</p><p><a href="https://deleter-d.github.io/posts/38038/">https://deleter-d.github.io/posts/38038/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><a href="https://deleter-d.github.io"><p>亦初</p></a></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2024-02-20</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2024-02-27</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a class="icon" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a><a class="icon" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="recommend-area"><div class="recommend-post"><span class="is-size-6 has-text-grey has-mr-7"># 相关文章</span><br><span>  1.<a class="is-size-6" href="/posts/9782/" target="_blank">CUDA编程——GPU加速库和OpenACC</a><br></span><span>  2.<a class="is-size-6" href="/posts/53610/" target="_blank">CUDA编程——调整指令集原语</a><br></span><span>  3.<a class="is-size-6" href="/posts/4919/" target="_blank">CUDA编程——流和并发</a><br></span><span>  4.<a class="is-size-6" href="/posts/50255/" target="_blank">CUDA编程——性能分析工具</a><br></span><span>  5.<a class="is-size-6" href="/posts/47184/" target="_blank">CUDA编程——全局内存</a><br></span><span>  6.<a class="is-size-6" href="/posts/47225/" target="_blank">CUDA编程——执行模型</a><br></span><span>  7.<a class="is-size-6" href="/posts/50741/" target="_blank">CUDA编程——NVCC编译器</a><br></span><span>  8.<a class="is-size-6" href="/posts/57516/" target="_blank">CUDA编程模型概述</a><br></span></div></div><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/social-share.js/1.0.16/css/share.min.css"><div class="social-share"></div><script src="https://cdnjs.loli.net/ajax/libs/social-share.js/1.0.16/js/social-share.min.js"></script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="https://user-images.githubusercontent.com/56388518/194691384-1d4515ba-79ae-4e83-a485-bfaa5c64033e.jpg" alt="支付宝"></span></a><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="https://user-images.githubusercontent.com/56388518/194691371-ad26d43d-b3b5-4fe5-9fc0-52a31333ca98.jpg" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/posts/50255/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">CUDA编程——性能分析工具</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/posts/47184/"><span class="level-item">CUDA编程——全局内存</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><!--!--><div class="card"><div class="card-content"><div class="title is-5">评论</div><div id="comment-container"></div><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/gitalk/1.6.0/gitalk.css"><script> $.getScript('/js/gitalk.min.js', function () { 
            var gitalk = new Gitalk({
            language:'zh-CN',
            id: '786495b5caba091c763264f572fdbb0b',
            repo: 'Deleter-D.github.io',
            owner: 'Deleter-D',
            clientID: 'd087baa8a532e3b31fba',
            clientSecret: 'faec4c1d7046247c200bca62c9930d0799ce58a5',
            admin: ["Deleter-D"],
            createIssueManually: true,
            distractionFreeMode: false,
            perPage: 10,
            pagerDirection: 'last',
            proxy: 'https://cloudflare-cors-anywhere.wyp867909454.workers.dev/?https://github.com/login/oauth/access_token',
            
            enableHotKey: true,
            isLocked: false
        })
        gitalk.render('comment-container')});</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">目录</h3><ul class="menu-list"><li><a class="is-flex is-mobile" href="#共享内存概述"><span>共享内存概述</span></a><ul class="menu-list"><li><a class="is-flex is-mobile" href="#共享内存"><span>共享内存</span></a></li><li><a class="is-flex is-mobile" href="#分布式共享内存"><span>分布式共享内存</span></a></li><li><a class="is-flex is-mobile" href="#共享内存的分配"><span>共享内存的分配</span></a></li><li><a class="is-flex is-mobile" href="#共享内存存储体和访问模式"><span>共享内存存储体和访问模式</span></a><ul class="menu-list"><li><a class="is-flex is-mobile" href="#内存存储体"><span>内存存储体</span></a></li><li><a class="is-flex is-mobile" href="#存储体冲突"><span>存储体冲突</span></a></li><li><a class="is-flex is-mobile" href="#访问模式"><span>访问模式</span></a></li><li><a class="is-flex is-mobile" href="#内存填充"><span>内存填充</span></a></li></ul></li><li><a class="is-flex is-mobile" href="#配置共享内存容量"><span>配置共享内存容量</span></a></li><li><a class="is-flex is-mobile" href="#同步"><span>同步</span></a><ul class="menu-list"><li><a class="is-flex is-mobile" href="#弱排序内存模型"><span>弱排序内存模型</span></a></li><li><a class="is-flex is-mobile" href="#显式障碍"><span>显式障碍</span></a></li><li><a class="is-flex is-mobile" href="#内存栅栏"><span>内存栅栏</span></a></li><li><a class="is-flex is-mobile" href="#在cuda中使用域"><span>在CUDA中使用域</span></a></li><li><a class="is-flex is-mobile" href="#volatile修饰符"><span>volatile修饰符</span></a></li></ul></li></ul></li><li><a class="is-flex is-mobile" href="#共享内存的数据布局"><span>共享内存的数据布局</span></a><ul class="menu-list"><li><a class="is-flex is-mobile" href="#方形共享内存"><span>方形共享内存</span></a><ul class="menu-list"><li><a class="is-flex is-mobile" href="#访问方式实例"><span>访问方式实例</span></a></li><li><a class="is-flex is-mobile" href="#动态共享内存"><span>动态共享内存</span></a></li><li><a class="is-flex is-mobile" href="#填充静态声明的共享内存"><span>填充静态声明的共享内存</span></a></li><li><a class="is-flex is-mobile" href="#填充动态声明的共享内存"><span>填充动态声明的共享内存</span></a></li></ul></li><li><a class="is-flex is-mobile" href="#矩形共享内存"><span>矩形共享内存</span></a></li></ul></li><li><a class="is-flex is-mobile" href="#减少全局内存访问"><span>减少全局内存访问</span></a><ul class="menu-list"><li><a class="is-flex is-mobile" href="#使用共享内存的并行归约"><span>使用共享内存的并行归约</span></a></li><li><a class="is-flex is-mobile" href="#使用展开的并行归约"><span>使用展开的并行归约</span></a></li><li><a class="is-flex is-mobile" href="#使用动态共享内存的并行归约"><span>使用动态共享内存的并行归约</span></a></li></ul></li><li><a class="is-flex is-mobile" href="#合并的全局内存访问"><span>合并的全局内存访问</span></a><ul class="menu-list"><li><a class="is-flex is-mobile" href="#朴素转置"><span>朴素转置</span></a></li><li><a class="is-flex is-mobile" href="#使用共享内存的矩阵转置"><span>使用共享内存的矩阵转置</span></a></li><li><a class="is-flex is-mobile" href="#使用填充共享内存的矩阵转置"><span>使用填充共享内存的矩阵转置</span></a></li><li><a class="is-flex is-mobile" href="#使用展开的矩阵转置"><span>使用展开的矩阵转置</span></a></li></ul></li><li><a class="is-flex is-mobile" href="#常量内存"><span>常量内存</span></a><ul class="menu-list"><li><a class="is-flex is-mobile" href="#使用常量内存实现一维模板"><span>使用常量内存实现一维模板</span></a></li><li><a class="is-flex is-mobile" href="#与只读缓存比较"><span>与只读缓存比较</span></a></li></ul></li><li><a class="is-flex is-mobile" href="#线程束洗牌指令"><span>线程束洗牌指令</span></a><ul class="menu-list"><li><a class="is-flex is-mobile" href="#线程束洗牌指令的不同形式"><span>线程束洗牌指令的不同形式</span></a><ul class="menu-list"><li><a class="is-flex is-mobile" href="#广播传递"><span>广播传递</span></a></li><li><a class="is-flex is-mobile" href="#向上传递和向下传递"><span>向上传递和向下传递</span></a></li><li><a class="is-flex is-mobile" href="#异或传递"><span>异或传递</span></a></li></ul></li></ul></li></ul></div></div><style>.menu-list > li > a.is-active + .menu-list { display: block; }.menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="https://user-images.githubusercontent.com/56388518/193990104-d040c2b4-1b96-4636-b410-b2ccd6360665.jpg" alt="亦初"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">亦初</p><p class="is-size-6 is-block">落霞与孤鹜齐飞，秋水共长天一色</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>冰岛</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">74</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">22</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">72</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/Deleter-D" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/Deleter-D"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Email" href="mailto:18735855248@163.com"><i class="fa fa-envelope"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Telegram" href="https://t.me/GoldPancake687"><i class="fab fa-telegram"></i></a></div><div><hr><p id="hitokoto">:D 一言句子获取中...</p><script type="text/javascript" defer>function getYiyan(){
                                $.getJSON("https://v1.hitokoto.cn/", function (data) {
                                if(data){
                                    $('#hitokoto').html("");
                                    $('#hitokoto').append("<strong style='color: #3273dc;'>"+data.hitokoto+"</strong>"+
                                    "<p>"+"来源《"+data.from+"》</p><p>提供者-"+data.creator+"</p>");
                                }});}
                                $(function (){getYiyan();$('#hitokoto').click(function(){getYiyan();})});</script></div></div></div><div class="card widget"><div class="card-content"><h3 class="menu-label">最新评论</h3><span class="body_hot_comment">加载中，最新评论有1分钟缓存...</span></div></div><div class="card widget"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-02-20T08:49:53.000Z">2024-02-20</time></p><p class="title"><a href="/posts/9782/">CUDA编程——GPU加速库和OpenACC</a></p><p class="categories"><a href="/categories/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/">高性能计算</a> / <a href="/categories/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/CUDA/">CUDA</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-02-20T08:45:21.000Z">2024-02-20</time></p><p class="title"><a href="/posts/53610/">CUDA编程——调整指令集原语</a></p><p class="categories"><a href="/categories/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/">高性能计算</a> / <a href="/categories/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/CUDA/">CUDA</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-02-20T08:36:28.000Z">2024-02-20</time></p><p class="title"><a href="/posts/4919/">CUDA编程——流和并发</a></p><p class="categories"><a href="/categories/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/">高性能计算</a> / <a href="/categories/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/CUDA/">CUDA</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-02-20T08:29:56.000Z">2024-02-20</time></p><p class="title"><a href="/posts/50255/">CUDA编程——性能分析工具</a></p><p class="categories"><a href="/categories/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/">高性能计算</a> / <a href="/categories/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/CUDA/">CUDA</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-02-20T08:22:30.000Z">2024-02-20</time></p><p class="title"><a href="/posts/38038/">CUDA编程——共享内存和常量内存</a></p><p class="categories"><a href="/categories/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/">高性能计算</a> / <a href="/categories/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/CUDA/">CUDA</a></p></div></article></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/categories/Hexo/"><span class="level-start"><span class="level-item">Hexo</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Java/"><span class="level-start"><span class="level-item">Java</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Linux/"><span class="level-start"><span class="level-item">Linux</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/%E5%89%8D%E7%AB%AF/"><span class="level-start"><span class="level-item">前端</span></span><span class="level-end"><span class="level-item tag">15</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/%E5%89%8D%E7%AB%AF/Vue/"><span class="level-start"><span class="level-item">Vue</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/%E5%89%8D%E7%AB%AF/axios/"><span class="level-start"><span class="level-item">axios</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/%E5%89%8D%E7%AB%AF/nodejs/"><span class="level-start"><span class="level-item">nodejs</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></li><li><a class="level is-mobile is-marginless" href="/categories/%E6%8A%98%E8%85%BE/"><span class="level-start"><span class="level-item">折腾</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/%E6%95%B0%E5%AD%A6/"><span class="level-start"><span class="level-item">数学</span></span><span class="level-end"><span class="level-item tag">6</span></span></a><ul class="mr-0"><li><a class="level is-mobile is-marginless" href="/categories/%E6%95%B0%E5%AD%A6/%E7%9F%A9%E9%98%B5%E8%AE%BA/"><span class="level-start"><span class="level-item">矩阵论</span></span><span class="level-end"><span class="level-item tag">6</span></span></a></li></ul></li><a class="level is-mobile is-marginless" href="/categories/"><span class="level-start"><span class="level-item">查看全部&gt;&gt;</span></span></a></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/archives/2024/02/"><span class="level-start"><span class="level-item">二月 2024</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2023/12/"><span class="level-start"><span class="level-item">十二月 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2023/11/"><span class="level-start"><span class="level-item">十一月 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2023/10/"><span class="level-start"><span class="level-item">十月 2023</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2023/08/"><span class="level-start"><span class="level-item">八月 2023</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><a class="level is-mobile is-marginless" href="/archives/"><span class="level-start"><span class="level-item">查看全部&gt;&gt;</span></span></a></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/%E7%AE%97%E6%B3%95/"><span class="tag">算法</span><span class="tag is-grey-lightest">11</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%BC%82%E6%9E%84%E8%AE%A1%E7%AE%97/"><span class="tag">异构计算</span><span class="tag is-grey-lightest">10</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CUDA/"><span class="tag">CUDA</span><span class="tag is-grey-lightest">9</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><span class="tag">机器学习</span><span class="tag is-grey-lightest">9</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%AF%95%E6%98%87%E7%BC%96%E8%AF%91%E5%99%A8/"><span class="tag">毕昇编译器</span><span class="tag is-grey-lightest">9</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/"><span class="tag">高性能计算</span><span class="tag is-grey-lightest">9</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Vue/"><span class="tag">Vue</span><span class="tag is-grey-lightest">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%8A%9B%E6%89%A3/"><span class="tag">力扣</span><span class="tag is-grey-lightest">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%95%B0%E5%AD%A6/"><span class="tag">数学</span><span class="tag is-grey-lightest">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%9F%A9%E9%98%B5%E8%AE%BA/"><span class="tag">矩阵论</span><span class="tag is-grey-lightest">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"><span class="tag">设计模式</span><span class="tag is-grey-lightest">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/axios/"><span class="tag">axios</span><span class="tag is-grey-lightest">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%BC%82%E6%9E%84%E7%BC%96%E7%A8%8B/"><span class="tag">异构编程</span><span class="tag is-grey-lightest">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/3D%E4%BA%BA%E4%BD%93%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/"><span class="tag">3D人体姿态估计</span><span class="tag is-grey-lightest">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Pytorch/"><span class="tag">Pytorch</span><span class="tag is-grey-lightest">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AE%BA%E6%96%87/"><span class="tag">论文</span><span class="tag is-grey-lightest">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ITK/"><span class="tag">ITK</span><span class="tag is-grey-lightest">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"><span class="tag">深度学习</span><span class="tag is-grey-lightest">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/CVPR/"><span class="tag">CVPR</span><span class="tag is-grey-lightest">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Hexo/"><span class="tag">Hexo</span><span class="tag is-grey-lightest">2</span></a></div></div><div class="field is-grouped is-grouped-multiline"><a class="tags has-addons" href="/tags/"><span class="tag">查看全部&gt;&gt;</span></a></div></div></div></div><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">链接</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li><li><a class="level is-mobile" href="https://bulma.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Bulma</span></span><span class="level-right"><span class="level-item tag">bulma.io</span></span></a></li></ul></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">订阅更新</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="订阅"></div></div></form></div></div></div><!--!--><div class="column-right-shadow is-hidden-widescreen"></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="https://user-images.githubusercontent.com/56388518/193990104-d040c2b4-1b96-4636-b410-b2ccd6360665.jpg" alt="亦初" height="28"></a><p class="size-small"><span>&copy; 2024 亦初</span>  Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank">Icarus</a> &amp; <a href="https://github.com/removeif/hexo-theme-amazing" target="_blank">Amazing</a> <br><span>© 版权说明：[本网站所有内容均收集于互联网或自己创作，如有侵权，请<a href="/message" target="_blank">留言</a>，笔者会立即处理]<br /></span><span><span id="statistic-times">loading...</span><script>function createTime(time) {
            var n = new Date(time);
            now.setTime(now.getTime() + 250),
                days = (now - n) / 1e3 / 60 / 60 / 24,
                dnum = Math.floor(days),
                hours = (now - n) / 1e3 / 60 / 60 - 24 * dnum,
                hnum = Math.floor(hours),
            1 == String(hnum).length && (hnum = "0" + hnum),
                minutes = (now - n) / 1e3 / 60 - 1440 * dnum - 60 * hnum,
                mnum = Math.floor(minutes),
            1 == String(mnum).length && (mnum = "0" + mnum),
                seconds = (now - n) / 1e3 - 86400 * dnum - 3600 * hnum - 60 * mnum,
                snum = Math.round(seconds),
            1 == String(snum).length && (snum = "0" + snum),
                document.getElementById("statistic-times").innerHTML = "❤️本站自 <strong>"+time.split(" ")[0].replace(/\//g,".")+"</strong> 已运行 <strong>" + dnum + "</strong> 天 <strong>" + hnum + "</strong> 小时 <strong>" + mnum + "</strong> 分 <strong>" + snum + "</strong> 秒！❤️";
        }var now = new Date();setInterval("createTime('2022/03/13 00:00:00')", 250,"");</script><br></span><div class="size-small"><span>❤️感谢 <strong><span id="busuanzi_value_site_uv">99+</span></strong> 小伙伴的 <strong><span id="busuanzi_value_site_pv">99+</span></strong> 次光临！❤️</span></div></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p></div><div class="sideMusic"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="/js/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script><meting-js style="width: auto;height: 2000px;" server="netease" type="playlist" id="475747480" theme="#2980b9" loop="all" autoplay="false" order="list" storageName="aplayer-setting" lrctype="0" list-max-height="400px" fixed="true"></meting-js></div></div></div></div></footer><script src="https://cdnjs.loli.net/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script><script src="https://cdnjs.loli.net/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" async></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdnjs.loli.net/ajax/libs/lightgallery/1.6.8/js/lightgallery.min.js" defer></script><script src="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.7.0/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdnjs.loli.net/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><script src="/js/main.js" defer></script><script>$.getScript('/js/comment-issue-data.js',function(){loadIssueData('d087baa8a532e3b31fba','faec4c1d7046247c200bca62c9930d0799ce58a5','Deleter-D','Deleter-D.github.io',false);})</script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.js"></script><script type="text/javascript">var pjax = new Pjax({
            elements: "a",//代表点击链接就更新
            selectors: [  //代表要更新的节点
                ".section",
                "title"
            ],
            cache: true,
            cacheBust:false
        })

        function loadBusuanzi(){
        $.getScript("//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js", function () {});
        }

        function loadMathJax() { //加载mathjax
            $.getScript("//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-MML-AM_CHTML", function () {
                MathJax.Hub.Config({ tex2jax: { inlineMath: [['$', '$'], ['\(', '\)']] } });
                var math = document.getElementsByClassName("entry-content")[0];
                MathJax.Hub.Queue(["Typeset", MathJax.Hub, math]);
            });
        };

        // 开始 PJAX 执行的函数
        document.addEventListener('pjax:send', function () {
        });
        
        // PJAX 完成之后执行的函数，可以和上面的重载放在一起
        document.addEventListener('pjax:complete', function () {
            $(".section").css({opacity:1});
            if(true){
                $.getScript('/js/comment-issue-data.js',function(){loadIssueData('d087baa8a532e3b31fba','faec4c1d7046247c200bca62c9930d0799ce58a5','Deleter-D','Deleter-D.github.io',false);});
            }
            if(false){
                loadMathJax();
            }
            loadMainJs(jQuery, window.moment, window.ClipboardJS, window.IcarusThemeSettings);
            loadBackTop();
            loadBusuanzi();
            if(typeof loadBanner == 'function'){
                loadBanner();
            }
        });</script></body></html>